<h1>Deep Learning in Daily US Equity Quantitative Analysis (2020–2025)</h1>
<b>Over the past five years, deep learning has become integral to quantitative analysis of U.S. equities (daily frequency). Below, we review recent developments in four key areas – Stock Price Prediction, Entry/Exit Signals, Portfolio Optimization, and Risk Management – focusing exclusively on deep learning models. For each area, we outline the models/architectures used, dataset and feature choices, training/evaluation practices, and notable academic and industry applications.
1. Stock Price Prediction</b>
<ol>
 <li>
  <h2>Stock Price Prediction</h2>
  <b>Stock price prediction refers to forecasting future price movements or returns of equities on a daily timeframe. Deep learning models have been extensively applied to this task, treating it as either a regression (predicting next-day price/return) or classification (up/down movement) problem. These models aim to capture complex non-linear patterns in historical market data that simpler models miss.</b>
  <ul>
   <li><b>Deep Learning Models & Architectures:</b> Early works relied on recurrent neural networks, especially LSTM and GRU networks, to model sequential price data. LSTM-based models can retain long-term dependencies in financial time series and have often outperformed basic CNN or feed-forward networks in trend prediction​. Convolutional Neural Networks (CNNs) have also been used, either on raw price sequences or technical indicator “images,” to extract local temporal patterns. For example, a CNN-based model (CNNpred) using a rich set of input features showed improved accuracy in market direction forecasting​. Temporal Convolutional Networks (TCN) – a 1D CNN architecture with dilated causal convolutions – have been introduced more recently to handle long historical windows; one 2022 study used TCNs to forecast daily stock index volatility and Value-at-Risk with success​ (indicating TCN’s potential for price trend prediction as well). The last few years have also seen Transformers (attention-based sequence models) adapted for stock prediction. Transformers can capture long-range dependencies and global context more effectively than RNNs​. Researchers have applied Transformer models to daily stock prices and achieved promising results in trend classification and regression tasks​. Specialized variants like the “Market-Guided Stock Transformer (MASTER)” incorporate market structure priors and have outperformed LSTM baselines​. Furthermore, Graph Neural Networks (GNNs) have emerged for modeling relationships among multiple stocks. By treating stocks as nodes in a graph (connected by industry, correlations, etc.), GNNs can learn inter-stock influences. Recent works (2023–2024) proposed dynamic graph networks that leverage stock relationship graphs (including news or correlation links) to predict price movements more comprehensively​. Finally, hybrid architectures are common – e.g. CNN+LSTM combinations with attention mechanisms​ – and even generative models have been explored (a 2024 study proposed a GAN-based multi-graph network to generate enriched features for price forecasting​).</li>
   <li><b>Input Data & Features:</b> Daily OHLCV price data (open, high, low, close, volume) is typically the foundational input. These raw time series are often augmented with technical indicators that summarize recent price patterns or momentum (e.g. moving averages, RSI, MACD, stochastic oscillators). For example, an attention-enhanced BiLSTM model was shown to benefit from input features like the stochastic oscillator, RSI, Williams %R, and MACD​. Some models incorporate fundamental data (e.g. earnings or valuation metrics) or macro-economic indicators, though such features are less common at daily frequency. A notable trend is integrating sentiment and textual data: news articles, social media sentiment (Twitter, Reddit), and even corporate disclosures can be converted to sentiment scores or topic features and fed into deep models. Studies have found that adding sentiment features improves predictive performance​. For instance, one 2024 model uses a sentiment-aware network to fuse news text with price data for stock index prediction​. In practice, large-scale datasets are used – e.g. many academic works evaluate on years of S&P 500 or NASDAQ stock data, often spanning a decade or more of daily observations to capture multiple market cycles. Some research focuses on individual stocks or indices, while others train models in a cross-sectional manner (predicting many stocks’ movements simultaneously). In cross-sectional setups, features can include not only a stock’s own history but also lagged returns of related stocks or sector indices, feeding a broader view of market state into the model.</li>
   <li><b>Training Strategies:</b> Supervised learning is the norm for price prediction. Data is typically split into rolling time windows – e.g. training on 2005–2015 data, validating on 2016–2017, and testing on 2018–2019. Walk-forward (rolling) validation is used to account for time-dependence and non-stationarity (models may be retrained periodically as new data comes in). To address the non-i.i.d. nature of time series, researchers avoid random shuffling and instead use sequential training and testing to mimic real forecasting conditions. Loss functions depend on the task: mean squared error (MSE) or mean absolute error (MAE) are common for regression (predicting future prices or returns), while binary cross-entropy or multi-class losses are used for direction/trend classification. Some works optimize custom objectives that relate to trading, such as maximizing the Sharpe ratio or minimizing prediction error of sign (to get direction right). Evaluation metrics include statistical errors like RMSE and MAPE for price forecasts, as well as classification metrics (accuracy, F1-score) for direction prediction​. However, since the ultimate goal is often profitable trading, many studies also report financial metrics: e.g. cumulative returns achieved by a trading strategy using the predictions, Sharpe ratio of those returns, and maximum drawdown. Back-testing a simple strategy (buy if model predicts rise, sell/short if fall) on test data gives a more practical evaluation of the model’s utility. For instance, one study evaluated models on both predictive accuracy and a trading simulation, finding the transformer-based model outperformed an LSTM in both error rates and backtested returns​.</li>
   <li><b>Notable Studies and Applications:</b> The field saw a comprehensive survey by Jiang (2021) that summarized the explosion of deep learning in stock prediction up to that point​, highlighting that LSTM and CNN variants were dominant and showing their edge over traditional methods. Since 2020, research has increasingly turned to attention mechanisms and multi-modal data. Hybrid frameworks are common – e.g. a 2023 study proposed an integrated model combining CNN, LSTM, and factor attention to capture different aspects of stock movement​. Another line of work uses unsupervised pretraining to extract features: Xie and Yu (2021) used a convolutional autoencoder on historical daily charts to learn latent features, which improved a subsequent prediction model​. In 2019, Hoseinzadeh & Haratizadeh introduced CNNpred, which leveraged a diverse set of 77 input variables (technical indicators, global cues, etc.) and a deep CNN to forecast stock trends​ – a representative example of engineering rich feature sets for deep models. More recently, graph-based models have set state-of-the-art in some benchmarks: e.g. ECHO-G (2024) built a heterogeneous graph of stocks and their earnings call transcripts to predict price jumps around earnings announcements​. In industry, AI-powered funds have adopted these techniques. For example, the AI Powered Equity ETF (AIEQ) employs IBM Watson’s deep learning and NLP algorithms to analyze 6,000+ US stocks daily, digesting news, social media, and financial statements to predict stock moves​. This fund, launched in 2017, demonstrated that an ensemble of deep models could manage a live portfolio (initially underperforming the market, but later matching and beating it as the models learned over time​). Hedge funds like Numerai also use deep learning at their core – Numerai crowdsources models from data scientists who train deep nets on encrypted market data to predict stock returns; the hedge fund then combines (“ensembles”) these models’ signals for trading​. Overall, deep learning has become a staple for daily stock prediction both in research and in practice, thanks to its ability to model complex patterns and incorporate varied data sources.</li>
  </ul>
 </li>
</ol>
