{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957febfa49dc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom libraries\n",
    "from Components.TrainModel import DataModule, TEMPUS, torchscript_predict\n",
    "from Components.TickerData import TickerData, upload_data_sql, fetch_sql_data\n",
    "from Components.BackTesting import BackTesting\n",
    "from Components.MarketRegimes import MarketRegimes\n",
    "\n",
    "# Torch ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b2e632fa4c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Feature importance with SHAP values and plot\n",
    "#TODO: Streamlit Page for future prediction\n",
    "#TODO: Add Williams %R to the TickerData\n",
    "#TODO:  Data is typically split into rolling time windows – e.g. training on 2005–2015 data, validating on 2016–2017, and testing on 2018–2019. Walk-forward (rolling) validation is used to account for time-dependence and non-stationarity (models may be retrained periodically as new data comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03f41dbce02e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "# Clean up the dataframe\n",
    "nasdaq_tickers = tickers.iloc[:, [1]].to_numpy().flatten()\n",
    "nasdaq_tickers = np.random.choice(nasdaq_tickers, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0eefdc3102151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/Russell_1000_Index\")[3]\n",
    "# Clean up the dataframe\n",
    "rusell_tickers = tickers.iloc[:, [1]].to_numpy().flatten()\n",
    "rusell_tickers = np.random.choice(rusell_tickers, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adf0cd99f26b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "# Clean up the dataframe\n",
    "SnP_tickers = tickers.iloc[:, [0]].to_numpy().flatten()\n",
    "SnP_tickers = np.random.choice(SnP_tickers, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68048b3e2c40ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_600_companies\")[0]\n",
    "# Clean up the dataframe\n",
    "SnP600_tickers = tickers.iloc[:, [0]].to_numpy().flatten()\n",
    "SnP600_tickers = np.random.choice(SnP600_tickers, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5a9535f391ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = np.concatenate((nasdaq_tickers, SnP_tickers,rusell_tickers,SnP600_tickers))\n",
    "tickers = np.unique(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c702a985a3b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tickers = ['IONQ','QBTS','RGTI']\n",
    "training_dfs = []\n",
    "stocks_dfs = []\n",
    "indicators = ['ema_20', 'ema_50', 'ema_200', 'stoch_rsi', 'macd', 'b_percent', 'keltner_lower', 'keltner_upper','adx','pcf',\n",
    "              'dte','roe','roa','pts','pe','eps_surprise','Close']\n",
    "for ticker in tickers:\n",
    "    training_data, raw_stock_data = TickerData(ticker,years=2,prediction_window=5,indicator_list=indicators).process_all()\n",
    "    training_dfs.append(training_data)\n",
    "    stocks_dfs.append(raw_stock_data)\n",
    "\n",
    "training_data = pd.concat(training_dfs, ignore_index=False)\n",
    "#stock_data = pd.concat(stocks_dfs, ignore_index=False)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220f8f82e25073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best config: {'lr': 4.390449033248878e-05, 'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3477694988633191, 'weight_decay': 0.0001801390872725824, 'batch_size': 16, 'window_size': 10, 'grad_clip_norm': 0.8393802881451728}\n",
    "\n",
    "config = {\n",
    "    \"lr\": 4.390449033248878e-05,\n",
    "    \"weight_decay\": 0.0001801390872725824,\n",
    "    \"hidden_size\": 256, # old was 256\n",
    "    \"num_layers\": 1, # old was 1\n",
    "    \"dropout\": 0.3477694988633191,\n",
    "    \"batch_size\": 16, # old was 16\n",
    "    \"window_size\": 5,\n",
    "    \"clip_size\": 0.8393802881451728,\n",
    "    \"attention_heads\": 4, #Deepseek R1 uses 128\n",
    "    \"epochs\": 20,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "}\n",
    "\n",
    "data_module = DataModule(training_data, window_size=config[\"window_size\"], batch_size=config[\"batch_size\"])\n",
    "config[\"input_size\"] = data_module.num_features\n",
    "\n",
    "# Instantiate the model\n",
    "model = TEMPUS(config,scaler=data_module.scaler)\n",
    "# Train Model\n",
    "history = model.train_model(data_module.train_loader, data_module.val_loader, data_module.test_loader, config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59105661d47ca1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fig = model.plot_training_history()\n",
    "training_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795551cdcf0cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained TEMPUS model\n",
    "script_path = model.export_model_to_torchscript(\n",
    "    save_path=\"Models/Echo_v1.0.pt\",\n",
    "    data_loader=data_module.test_loader,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cab10f186281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Randomly sample 50 tickers from the SnP600_tickers list\n",
    "sampled_tickers = random.sample(list(nasdaq_tickers), 10)\n",
    "initial_capital = 1000.0\n",
    "\n",
    "preds_dfs = []\n",
    "returns = []\n",
    "for idx, ticker in enumerate(sampled_tickers, start=1):\n",
    "    out_of_sample_data, raw_stock_data = TickerData(ticker, years=4, prediction_window=5).process_all()\n",
    "\n",
    "    # Check if raw_stock_data is NoneType, if so, skip this iteration\n",
    "    if out_of_sample_data is not None:\n",
    "        # Load the model and make predictions\n",
    "        preds_df = torchscript_predict(\n",
    "            model_path=\"Models/Tempus_v2.1.pt\",\n",
    "            input_df=out_of_sample_data,\n",
    "            device=\"cpu\",\n",
    "            window_size=50,\n",
    "            target_col=\"shifted_prices\"\n",
    "        )\n",
    "        preds_df = pd.merge(preds_df, raw_stock_data[['Open', 'High', 'Low', 'Volume','Close']], left_index=True, right_index=True, how='left')\n",
    "        preds_dfs.append(preds_df)\n",
    "\n",
    "        backtester = BackTesting(preds_df, ticker, initial_capital, pct_change_entry=0.05, pct_change_exit=0.03)\n",
    "        backtester.run_simulation()\n",
    "        bt_results = pd.DataFrame(backtester.pf.returns())\n",
    "        bt_results['cumulative_return'] = np.array(((1 + bt_results[0]).cumprod() - 1)*100)\n",
    "        bt_results['ticker'] = ticker\n",
    "        returns.append(bt_results)\n",
    "\n",
    "preds_dfs = pd.concat(preds_dfs, ignore_index=False)\n",
    "returns = pd.concat(returns, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5c425584ea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf8887699b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753113118c0c08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns for each ticker and visualize them using Plotly\n",
    "# Group data by 'ticker' and calculate cumulative returns\n",
    "\n",
    "# Create an interactive plot using Plotly\n",
    "fig = px.line(\n",
    "    returns.reset_index(),\n",
    "    x='index',\n",
    "    y='cumulative_return',\n",
    "    color='ticker',\n",
    "    title='Cumulative Returns by Ticker',\n",
    "    labels={'index': 'Date', 'cumulative_return': 'Cumulative Return'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Cumulative Return (%)',\n",
    "    showlegend=False,\n",
    "    height=600,\n",
    "    template='ggplot2',\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible=False),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "last_returns = returns.groupby('ticker')['cumulative_return'].last()\n",
    "\n",
    "# Count positive and negative returns\n",
    "positive_count = sum(last_returns > 0)\n",
    "negative_count = sum(last_returns <= 0)\n",
    "total_count = len(last_returns)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "last_returns_df = pd.DataFrame(last_returns).reset_index()\n",
    "last_returns_df.columns = ['Ticker', 'Final Return']\n",
    "last_returns_df.sort_values('Final Return', ascending=False, inplace=True)\n",
    "\n",
    "# Create a simple pie chart showing the proportion\n",
    "fig_pie = px.pie(\n",
    "    values=[positive_count, negative_count],\n",
    "    names=['Positive', 'Negative'],\n",
    "    title='Proportion of Tickers with Positive vs Negative Returns',\n",
    "    color_discrete_sequence=['green', 'red'],\n",
    "    template='ggplot2',\n",
    ")\n",
    "\n",
    "fig_pie.update_traces(textinfo='percent+label').update_layout(showlegend=False)\n",
    "fig_pie.show()\n",
    "\n",
    "# Calculate the proportion of tickers with positive returns\n",
    "if total_count > 0:\n",
    "    positive_proportion = positive_count / total_count\n",
    "    print(f\"Proportion of tickers with positive cumulative returns: {positive_proportion:.2%}\")\n",
    "    print(f\"Positive tickers: {positive_count} out of {total_count}\")\n",
    "    print(f\"Negative tickers: {negative_count} out of {total_count}\")\n",
    "else:\n",
    "    print(\"No ticker data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9e16de473b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a ticker from the `preds_dfs` object\n",
    "selected_ticker = random.choice(preds_dfs['Ticker'].unique())\n",
    "\n",
    "# Filter the `preds_dfs` DataFrame for the selected ticker\n",
    "preds_df = preds_dfs[preds_dfs['Ticker'] == selected_ticker]\n",
    "\n",
    "# Update the plot to reflect the filtered data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=preds_df['Predicted'], x=preds_df.index, mode='lines', name='Predicted', line=dict(color=\"Grey\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['Close'], x=preds_df.index, mode='lines', name='Close (Unshifted)', line=dict(color=\"Blue\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['Actual'], x=preds_df.index, mode='lines', name='Close (Shifted)'))\n",
    "fig.update_layout(\n",
    "    title=f'Prediction for {selected_ticker}',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (USD)',\n",
    "    height=600,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02),\n",
    "    template='ggplot2'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828915f2e50dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Components.BackTesting import BackTesting\n",
    "import pandas as pd\n",
    "ticker = 'PLTR'\n",
    "out_of_sample_data, raw_stock_data = TickerData(ticker, years=1, prediction_window=5,prediction_mode=True).process_all()\n",
    "\n",
    "preds_df = torchscript_predict(\n",
    "    model_path=\"Models/Tempus_v2.2.pt\",\n",
    "    input_df=out_of_sample_data,\n",
    "    device=\"cpu\",\n",
    "    window_size=50,\n",
    "    prediction_mode=True\n",
    ")\n",
    "preds_df = pd.merge(preds_df, raw_stock_data[['Open', 'High', 'Low', 'Volume','Close']], left_index=True, right_index=True, how='left')\n",
    "preds_df['shifted_prices'] = preds_df['Close'].shift(-abs(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02af7371fd116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=preds_df['Predicted'], x=preds_df.index, mode='lines', name='Predicted', line=dict(color=\"Grey\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['shifted_prices'], x=preds_df.index, mode='lines', name='Close (Shifted)', line=dict(color=\"Blue\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['Close'], x=preds_df.index, mode='lines', name='Close (Unshifted)', line=dict(color=\"Orange\")))\n",
    "fig.update_layout(template='ggplot2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b549aa49dad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "backtester = BackTesting(preds_df, ticker, initial_capital, pct_change_entry=0.05,pct_change_exit=0.02)\n",
    "backtester.run_simulation()\n",
    "returns = backtester.pf.returns()\n",
    "returns.index = returns.index.tz_localize(None)\n",
    "\n",
    "#html = qs.reports.full(returns, \"NDAQ\")\n",
    "qs.reports.basic(returns, \"PLTR\",rf=0.0025, display=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a00dbfc5aa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_data = ticker_obj.get_earnings_dates()\n",
    "earnings_data = earnings_data.reset_index().rename(\n",
    "    columns={\"Earnings Date\": \"Date\", \"EPS Estimate\": \"eps_estimate\", \"Reported EPS\": \"eps\",\n",
    "             \"Surprise(%)\": \"eps_surprise\"}).sort_values('Date')\n",
    "earnings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a744bc88e1bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "ticker_obj = yf.Ticker('CCEP')\n",
    "q_income_stmt = ticker_obj.get_income_stmt(freq='quarterly').T\n",
    "q_income_stmt = q_income_stmt.reset_index().rename(columns={\"index\": \"Date\"}).sort_values('Date')\n",
    "\n",
    "q_balance_sheet = ticker_obj.get_balance_sheet(freq='quarterly').T\n",
    "q_balance_sheet = q_balance_sheet.reset_index().rename(columns={\"index\": \"Date\"}).sort_values('Date')\n",
    "\n",
    "# Combine all metrics into a DataFrame\n",
    "financial_metrics = pd.DataFrame({\n",
    "    'ttm_eps' : q_income_stmt['NetIncome'] / q_income_stmt['BasicAverageShares'],\n",
    "    'pcf': q_balance_sheet['TotalCapitalization'] / q_income_stmt['OperatingIncome'],\n",
    "    'dte': q_balance_sheet['CurrentLiabilities'] / q_balance_sheet['StockholdersEquity'],\n",
    "    'roe': q_income_stmt['NetIncome'] / q_balance_sheet['StockholdersEquity'],\n",
    "    'roa': q_income_stmt['NetIncome'] / q_balance_sheet['TotalAssets'],\n",
    "    'pts': q_balance_sheet['TotalCapitalization'] / q_income_stmt['TotalRevenue'],\n",
    "    'evEBITDA': (q_balance_sheet['TotalCapitalization'] + q_balance_sheet['TotalDebt'] - q_balance_sheet[\n",
    "    'CashAndCashEquivalents']) / q_income_stmt['EBITDA']\n",
    "})\n",
    "financial_metrics['Date'] = q_balance_sheet['Date'].dt.tz_localize('America/New_York')\n",
    "financial_metrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740ac2bb348512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon import RESTClient\n",
    "client = RESTClient(api_key=\"XizU4KyrwjCA6bxHrR5_eQnUxwFFUnI2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c243df",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.today()\n",
    "# List Aggregates (Bars)\n",
    "def get_ohlc(ticker, date):\n",
    "    aggs = []\n",
    "\n",
    "    for a in client.list_aggs(ticker=ticker, multiplier=1, timespan=\"day\", from_=\"2024-01-01\", to=date, limit=50000):\n",
    "        aggs.append(a)\n",
    "    aggs = pd.DataFrame(aggs)\n",
    "    \n",
    "    dt_utc = pd.to_datetime(aggs['timestamp'], unit=\"ms\", utc=True)\n",
    "    aggs['timestamp'] = dt_utc.dt.tz_convert('America/New_York')\n",
    "    aggs['ticker'] = ticker\n",
    "\n",
    "    return aggs\n",
    "\n",
    "stocks_dfs = []\n",
    "for ticker in tickers:\n",
    "    stock_df = get_ohlc(ticker, current_date)\n",
    "    stocks_dfs.append(stock_df)\n",
    "stocks_dfs = pd.concat(stocks_dfs, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.merge(preds_df, raw_stock_data[['Open', 'High', 'Low', 'Volume','Close']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18393a",
   "metadata": {},
   "source": [
    "financials = []\n",
    "current_date = datetime.today()\n",
    "past_date =  current_date - timedelta(days=(365*2))\n",
    "for f in client.vx.list_stock_financials(ticker, filing_date_lte=current_date.strftime(\"%Y-%m-%d\"),filing_date_gte=past_date.strftime(\"%Y-%m-%d\")):\n",
    "    financials.append(f)\n",
    "financials = pd.DataFrame(financials)\n",
    "\n",
    "import json   # only needed if your column contains JSON *strings*\n",
    "financials[\"financials\"] = financials[\"financials\"].apply(\n",
    "    lambda v: v if isinstance(v, dict) else json.loads(v)\n",
    ")\n",
    "flat = pd.json_normalize(\n",
    "    financials[\"financials\"].tolist()\n",
    ")\n",
    "flat_filtered = (\n",
    "    flat\n",
    "    .filter(like=\"value\") \n",
    "    #.dropna(axis=1, how=\"all\")   # drop cols that are all missing\n",
    ")\n",
    "flat_filtered.index = financials.index\n",
    "financials = financials.drop(columns=[\"financials\"]).join(flat_filtered)\n",
    "financials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def parse_and_flatten(cell):\n",
    "    if isinstance(cell, str):\n",
    "        try:\n",
    "            js = json.loads(cell)\n",
    "        except json.JSONDecodeError:\n",
    "            js = ast.literal_eval(cell)\n",
    "    else:\n",
    "        js = cell\n",
    "    out = []\n",
    "    if js is None:\n",
    "        return out\n",
    "    if isinstance(js, list):\n",
    "        for item in js:\n",
    "            row = {}\n",
    "            if isinstance(item, dict):\n",
    "                row.update(item)\n",
    "            else:\n",
    "                row['value'] = item\n",
    "            out.append(row)\n",
    "\n",
    "    return out\n",
    "\n",
    "def process_news_data(ticker, rolling_period=3):\n",
    "    try:\n",
    "        news = []\n",
    "        for n in client.list_ticker_news(ticker, order=\"desc\", limit=1000):\n",
    "            news.append(n)\n",
    "        news = pd.DataFrame(news)\n",
    "        \n",
    "        news['parsed'] = news['insights'].apply(parse_and_flatten)\n",
    "        df_exp = news.explode('parsed').reset_index(drop=False)\n",
    "        flat = pd.json_normalize(df_exp['parsed'])\n",
    "        df = pd.concat(\n",
    "            [ df_exp.drop(columns=['insights','parsed']), \n",
    "              flat\n",
    "            ], axis=1\n",
    "        )\n",
    "        \n",
    "        df[\"published_utc\"] = pd.to_datetime(df[\"published_utc\"], utc=True)\n",
    "        df[\"published_ny\"]  = df[\"published_utc\"].dt.tz_convert(\"America/New_York\")\n",
    "        df[\"date\"] = df[\"published_ny\"].dt.date\n",
    "        df[\"sentiment_val\"] = df[\"sentiment\"].map({\"positive\":1, \"neutral\":0, \"negative\":-1})\n",
    "        news = (\n",
    "            df\n",
    "            .groupby([\"date\", \"sentiment_val\"])\n",
    "            .size()                      # counts per (date, sentiment_val)\n",
    "            .unstack(fill_value=0)      # wide format: one column per sentiment_val\n",
    "            .rename(columns={\n",
    "                1:  \"positive_count\",\n",
    "                0:  \"neutral_count\",\n",
    "               -1: \"negative_count\"\n",
    "            })\n",
    "        )\n",
    "        for col in [\"positive_count\",\"neutral_count\",\"negative_count\"]:\n",
    "            if col not in news:\n",
    "                news[col] = 0\n",
    "        \n",
    "        # Reorder columns\n",
    "        news['ticker'] = ticker\n",
    "        news = news[[\"ticker\",\"positive_count\",\"neutral_count\",\"negative_count\"]]\n",
    "        news['total_count'] = news['positive_count'] + news['negative_count'] + news['neutral_count']\n",
    "        news['pos_sent_ratio'] = news['positive_count'] / news['total_count']\n",
    "        news['neg_sent_ratio'] = news['negative_count'] / news['total_count']\n",
    "        news['neu_sent_ratio'] = news['neutral_count'] / news['total_count']\n",
    "        news['net_sentiment'] = (news['positive_count'] - news['negative_count']) / news['total_count']\n",
    "        news['roll3_net'] = news['net_sentiment'].rolling(rolling_period).mean()\n",
    "        \n",
    "    except KeyError:\n",
    "                news = pd.DataFrame()\n",
    "    \n",
    "    return news\n",
    "\n",
    "news_dfs = []\n",
    "for ticker in tickers:\n",
    "    news_df = process_news_data(ticker, rolling_period=3)\n",
    "    news_dfs.append(news_df)\n",
    "\n",
    "news_dfs = pd.concat(news_dfs, ignore_index=False)\n",
    "news_dfs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ————————————\n",
    "# 1) Prepare your data\n",
    "# ————————————\n",
    "df_feat = news_dfs[[\"positive_count\",\"neutral_count\",\"negative_count\",\"total_count\",\"pos_sent_ratio\",\"neg_sent_ratio\",\"neu_sent_ratio\",\"net_sentiment\",\"roll3_net\"]]\n",
    "df_target\n",
    "\n",
    "X_all = df_feat.values         # shape (T, F)\n",
    "y_all = df_target.values       # shape (T,)\n",
    "\n",
    "# sliding window to build sequences\n",
    "def create_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 10\n",
    "X_seq, y_seq = create_sequences(X_all, y_all, SEQ_LEN)\n",
    "\n",
    "# train/test split (no shuffling because it’s time series)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# a simple Dataset & DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = TimeSeriesDataset(X_train, y_train)\n",
    "test_ds  = TimeSeriesDataset(X_test,  y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ————————————\n",
    "# 2) Define the LSTM model\n",
    "# ————————————\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        # take the last layer’s hidden state\n",
    "        h_last = hn[-1]               # shape (batch, hidden_dim)\n",
    "        return self.out(h_last).squeeze(-1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMRegressor(input_dim=X_all.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# ————————————\n",
    "# 3) Training loop\n",
    "# ————————————\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg = total_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch:2d} — Train MSE: {avg:.6f}\")\n",
    "\n",
    "\n",
    "# ————————————\n",
    "# 4) SHAP feature‐importance\n",
    "# ————————————\n",
    "# (a) wrap model’s predict into a numpy function\n",
    "def predict_np(x):\n",
    "    # x: numpy array [samples, seq_len, features]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(x).float().to(device)\n",
    "        out = model(t).cpu().numpy()\n",
    "    return out\n",
    "\n",
    "# (b) pick a small background set (<=100 samples)\n",
    "bg = X_train[np.random.choice(len(X_train), size=100, replace=False)]\n",
    "\n",
    "# (c) KernelExplainer  \n",
    "explainer = shap.KernelExplainer(predict_np, bg)\n",
    "\n",
    "# (d) explain first 50 test cases\n",
    "shap_vals = explainer.shap_values(X_test[:50])\n",
    "\n",
    "# shap_vals.shape == (50, seq_len, n_features)\n",
    "# collapse time axis by mean, then average abs\n",
    "mean_abs_time = np.mean(np.abs(shap_vals), axis=0)    # (seq_len, n_feat)\n",
    "feat_imp = mean_abs_time.mean(axis=0)                # (n_feat,)\n",
    "\n",
    "# (e) bar‐plot feature importances\n",
    "feature_names = df_feat.columns.tolist()  # adjust as needed\n",
    "plt.figure()\n",
    "plt.bar(range(len(feat_imp)), feat_imp)\n",
    "plt.xticks(range(len(feat_imp)), feature_names, rotation=90)\n",
    "plt.ylabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Feature importance (averaged over time)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac330bed196d3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.get_ticker_details(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6df037863c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# %%\n",
    "# Import stock_data dataframe into an Azure SQL database table using SQLAlchemy\n",
    "#upload_data_sql(stock_data,\"SNP600_1day\")\n",
    "#SNP500_1day = fetch_sql_data('SNP500_1day')\n",
    "#SNP600_1day = fetch_sql_data('SNP600_1day')\n",
    "#russell2000_1day = fetch_sql_data('russell2000_1day')\n",
    "#dowjones_1day = fetch_sql_data('dowjones_1day')\n",
    "#nasdaq_1day = fetch_sql_data('nasdaq_1day')\n",
    "#stock_data = pd.concat([SNP500_1day, SNP600_1day, dowjones_1day, nasdaq_1day], ignore_index=True)\n",
    "# Remove duplicates based on the 'Date' and 'Ticker' columns\n",
    "#stock_data = stock_data[~stock_data.index.duplicated(keep='first')]\n",
    "# Before conversion\n",
    "#print(\"Column types before:\", [type(col).__name__ for col in training_data.columns])\n",
    "\n",
    "# Apply conversion\n",
    "#training_data.columns = [str(col) for col in training_data.columns]\n",
    "\n",
    "# After conversion\n",
    "#print(\"Column types after:\", [type(col).__name__ for col in training_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c23c973b360185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab652b42b0cef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
