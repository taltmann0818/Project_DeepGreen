{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T02:02:22.672737Z",
     "start_time": "2025-03-31T02:02:17.062695Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom libraries\n",
    "from Components.TrainModel import DataModule, TEMPUS\n",
    "from Components.TickerData import TickerData\n",
    "from Components.BackTesting import BackTesting\n",
    "\n",
    "# Torch ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b00ae557-72e6-4a76-a93d-78e2381c05e0",
   "metadata": {},
   "source": [
    "#TODO: Feature importance with SHAP values and plot\n",
    "#TODO: hyperparameter tuning\n",
    "#TODO: buy signals become if prediction > current by some delta (~5%). Reverse is sell (decrease by some delta). Senstitvity analysis should be conducted to compare this delta level\n",
    "#TODO: Use quantstats for a HTMl tearsheet\n",
    "#TODO: market-regime detector with Hiden-markov model\n",
    "#TODO: Add a Echo State Networks (ESN) layer to the model\n",
    "#TODO: randomly sample 50 tickers, run backtest for all of them, and plot. take average sharpe ratio, and other metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc304bd6da39d466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T00:48:18.945759Z",
     "start_time": "2025-03-31T00:48:18.422496Z"
    }
   },
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "# Clean up the dataframe\n",
    "tickers = tickers.iloc[:, [1]].to_numpy().flatten()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1da234ad-7e05-4dd8-bc3c-9ea24c392f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T00:49:07.958121Z",
     "start_time": "2025-03-31T00:48:22.038849Z"
    }
   },
   "source": [
    "#tickers = ['IONQ','QBTS','RGTI']\n",
    "training_dfs = []\n",
    "stocks_dfs = []\n",
    "for ticker in tickers:\n",
    "    training_data, raw_stock_data = TickerData(ticker,years=10,prediction_window=5).process_all()\n",
    "    training_dfs.append(training_data)\n",
    "    stocks_dfs.append(raw_stock_data)\n",
    "\n",
    "training_data = pd.concat(training_dfs, ignore_index=False)\n",
    "stock_data = pd.concat(stocks_dfs, ignore_index=False)\n",
    "training_data"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while processing the data for CCEP\n",
      "Error while merging data for CCEP; error: \"['State'] not in index\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                          Ticker      ema_20      ema_50     ema_100  \\\n",
       "Date                                                                   \n",
       "2023-07-26 00:00:00-04:00   ADBE  510.347534  501.721580  498.307934   \n",
       "2023-07-27 00:00:00-04:00   ADBE  510.692528  502.201909  498.618074   \n",
       "2023-07-28 00:00:00-04:00   ADBE  512.423715  503.247716  499.217122   \n",
       "2023-07-31 00:00:00-04:00   ADBE  515.637646  504.930942  500.146881   \n",
       "2023-08-01 00:00:00-04:00   ADBE  518.824534  506.663061  501.116249   \n",
       "...                          ...         ...         ...         ...   \n",
       "2025-03-17 00:00:00-04:00     ZS  198.903952  198.439884  195.948238   \n",
       "2025-03-18 00:00:00-04:00     ZS  199.159766  198.563418  196.059956   \n",
       "2025-03-19 00:00:00-04:00     ZS  199.615978  198.774656  196.216195   \n",
       "2025-03-20 00:00:00-04:00     ZS  199.990647  198.961925  196.361419   \n",
       "2025-03-21 00:00:00-04:00     ZS  200.486776  199.206555  196.536440   \n",
       "\n",
       "                           stoch_rsi       macd  State       Close  \\\n",
       "Date                                                                 \n",
       "2023-07-26 00:00:00-04:00   0.722119   8.426182      0  514.549988   \n",
       "2023-07-27 00:00:00-04:00   0.573108   7.649928      0  513.969971   \n",
       "2023-07-28 00:00:00-04:00   0.366788   8.143179      0  528.869995   \n",
       "2023-07-31 00:00:00-04:00   0.364328   9.816886      0  546.169983   \n",
       "2023-08-01 00:00:00-04:00   0.372044  11.250053      0  549.099976   \n",
       "...                              ...        ...    ...         ...   \n",
       "2025-03-17 00:00:00-04:00   1.000000  -0.727792      0  202.649994   \n",
       "2025-03-18 00:00:00-04:00   0.913036  -0.414348      0  201.589996   \n",
       "2025-03-19 00:00:00-04:00   1.000000   0.024213      0  203.949997   \n",
       "2025-03-20 00:00:00-04:00   0.903473   0.335630      0  203.550003   \n",
       "2025-03-21 00:00:00-04:00   1.000000   0.707416      0  205.199997   \n",
       "\n",
       "                           shifted_prices  \n",
       "Date                                       \n",
       "2023-07-26 00:00:00-04:00      530.299988  \n",
       "2023-07-27 00:00:00-04:00      523.760010  \n",
       "2023-07-28 00:00:00-04:00      526.880005  \n",
       "2023-07-31 00:00:00-04:00      529.729980  \n",
       "2023-08-01 00:00:00-04:00      520.599976  \n",
       "...                                   ...  \n",
       "2025-03-17 00:00:00-04:00      209.869995  \n",
       "2025-03-18 00:00:00-04:00      215.729996  \n",
       "2025-03-19 00:00:00-04:00      211.550003  \n",
       "2025-03-20 00:00:00-04:00      209.449997  \n",
       "2025-03-21 00:00:00-04:00      207.139999  \n",
       "\n",
       "[42619 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>ema_20</th>\n",
       "      <th>ema_50</th>\n",
       "      <th>ema_100</th>\n",
       "      <th>stoch_rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>State</th>\n",
       "      <th>Close</th>\n",
       "      <th>shifted_prices</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-26 00:00:00-04:00</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>510.347534</td>\n",
       "      <td>501.721580</td>\n",
       "      <td>498.307934</td>\n",
       "      <td>0.722119</td>\n",
       "      <td>8.426182</td>\n",
       "      <td>0</td>\n",
       "      <td>514.549988</td>\n",
       "      <td>530.299988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27 00:00:00-04:00</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>510.692528</td>\n",
       "      <td>502.201909</td>\n",
       "      <td>498.618074</td>\n",
       "      <td>0.573108</td>\n",
       "      <td>7.649928</td>\n",
       "      <td>0</td>\n",
       "      <td>513.969971</td>\n",
       "      <td>523.760010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28 00:00:00-04:00</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>512.423715</td>\n",
       "      <td>503.247716</td>\n",
       "      <td>499.217122</td>\n",
       "      <td>0.366788</td>\n",
       "      <td>8.143179</td>\n",
       "      <td>0</td>\n",
       "      <td>528.869995</td>\n",
       "      <td>526.880005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 00:00:00-04:00</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>515.637646</td>\n",
       "      <td>504.930942</td>\n",
       "      <td>500.146881</td>\n",
       "      <td>0.364328</td>\n",
       "      <td>9.816886</td>\n",
       "      <td>0</td>\n",
       "      <td>546.169983</td>\n",
       "      <td>529.729980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01 00:00:00-04:00</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>518.824534</td>\n",
       "      <td>506.663061</td>\n",
       "      <td>501.116249</td>\n",
       "      <td>0.372044</td>\n",
       "      <td>11.250053</td>\n",
       "      <td>0</td>\n",
       "      <td>549.099976</td>\n",
       "      <td>520.599976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17 00:00:00-04:00</th>\n",
       "      <td>ZS</td>\n",
       "      <td>198.903952</td>\n",
       "      <td>198.439884</td>\n",
       "      <td>195.948238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.727792</td>\n",
       "      <td>0</td>\n",
       "      <td>202.649994</td>\n",
       "      <td>209.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18 00:00:00-04:00</th>\n",
       "      <td>ZS</td>\n",
       "      <td>199.159766</td>\n",
       "      <td>198.563418</td>\n",
       "      <td>196.059956</td>\n",
       "      <td>0.913036</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>0</td>\n",
       "      <td>201.589996</td>\n",
       "      <td>215.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19 00:00:00-04:00</th>\n",
       "      <td>ZS</td>\n",
       "      <td>199.615978</td>\n",
       "      <td>198.774656</td>\n",
       "      <td>196.216195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0</td>\n",
       "      <td>203.949997</td>\n",
       "      <td>211.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-20 00:00:00-04:00</th>\n",
       "      <td>ZS</td>\n",
       "      <td>199.990647</td>\n",
       "      <td>198.961925</td>\n",
       "      <td>196.361419</td>\n",
       "      <td>0.903473</td>\n",
       "      <td>0.335630</td>\n",
       "      <td>0</td>\n",
       "      <td>203.550003</td>\n",
       "      <td>209.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-21 00:00:00-04:00</th>\n",
       "      <td>ZS</td>\n",
       "      <td>200.486776</td>\n",
       "      <td>199.206555</td>\n",
       "      <td>196.536440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707416</td>\n",
       "      <td>0</td>\n",
       "      <td>205.199997</td>\n",
       "      <td>207.139999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42619 rows Ã— 9 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3d3bf5fc-fc97-42e8-b637-bb2dea141680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T00:49:21.268627Z",
     "start_time": "2025-03-31T00:49:18.830248Z"
    }
   },
   "source": [
    "training_data.to_csv(\"Data/NASDAQ_100_TrainingData_v2.1.csv\", index=True)\n",
    "stock_data.to_csv(\"Data/NASDAQ_100_StockData_v2.1.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "f39a8f78-fe7d-41e9-8e51-0a522e986489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:24:40.120552Z",
     "start_time": "2025-03-30T19:24:39.877726Z"
    }
   },
   "source": [
    "training_data = pd.read_csv(\"Data/NASDAQ_100_TrainingData_v2.1.csv\")\n",
    "training_data = training_data.set_index(training_data['Date']).drop(columns=['Date'])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e96a1096-cd23-467c-befe-284716d6b4d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:24:40.903632Z",
     "start_time": "2025-03-30T19:24:40.710236Z"
    }
   },
   "source": [
    "stock_data = pd.read_csv(\"Data/NASDAQ_100_StockData_v2.1.csv\")\n",
    "stock_data = stock_data.set_index(stock_data['Date']).drop(columns=['Date'])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "788c08f8808767bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T01:00:34.948550Z",
     "start_time": "2025-03-31T00:49:27.877985Z"
    }
   },
   "source": [
    "# Automatically get the number of features given my data_module object\n",
    "\n",
    "#Best config: {'lr': 4.390449033248878e-05, 'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3477694988633191, 'weight_decay': 0.0001801390872725824, 'batch_size': 16, 'window_size': 10, 'grad_clip_norm': 0.8393802881451728}\n",
    "\n",
    "config = {\n",
    "    \"lr\": 4.390449033248878e-05,\n",
    "    \"weight_decay\": 0.0001801390872725824,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.3477694988633191,\n",
    "    \"batch_size\": 16,\n",
    "    \"window_size\": 50,\n",
    "    \"clip_size\": 0.8393802881451728,\n",
    "    \"epochs\": 20,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "data_module = DataModule(training_data, window_size=config[\"window_size\"], batch_size=config[\"batch_size\"])\n",
    "config[\"input_size\"] = data_module.num_features\n",
    "\n",
    "# Instantiate the model\n",
    "model = TEMPUS(config)\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "# Train Model\n",
    "history = model.train_model(data_module.train_loader, data_module.test_loader, criterion, optimizer, config[\"epochs\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "513d70554ebe4221ba7184ec0ff2a5a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MAPE: 8.29%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T01:53:11.846554Z",
     "start_time": "2025-03-31T01:53:11.292738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def export_model_to_torchscript(model, save_path, data_loader, device):\n",
    "    \"\"\"\n",
    "    Exports a PyTorch model to TorchScript format.\n",
    "\n",
    "    Parameters:\n",
    "        model (nn.Module): The trained PyTorch model to export.\n",
    "        save_path (str): File name for the saved TorchScript model (including extension, e.g., 'TEMPUS_v3.pt').\n",
    "        data_loader (DataLoader): DataLoader containing sample input data to trace the model.\n",
    "        device (str): Device on which the model operates, e.g., 'cpu' or 'cuda'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Fetch a sample input tensor from DataLoader\n",
    "        example_inputs, _ = next(iter(data_loader))\n",
    "        example_inputs = example_inputs.to(device)\n",
    "\n",
    "        # Export model to TorchScript using tracing\n",
    "        scripted_model = torch.jit.trace(model.to(device), example_inputs)\n",
    "\n",
    "        # Save the TorchScript model\n",
    "        torch.jit.save(scripted_model, save_path)\n",
    "\n",
    "        print(f\"Model successfully exported and saved to {save_path}\")\n",
    "        return save_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting model to TorchScript: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Export the trained TEMPUS model\n",
    "script_path = export_model_to_torchscript(\n",
    "    model=model,\n",
    "    save_path=\"Models/Tempus_v2.pt\",\n",
    "    data_loader=data_module.test_loader,\n",
    "    device=config[\"device\"]\n",
    ")\n"
   ],
   "id": "ebd8c392578867a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Altmann\\PycharmProjects\\Project_DeepGreen\\Components\\TrainModel.py:80: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len % factor != 0:\n",
      "C:\\Users\\Thomas Altmann\\PycharmProjects\\Project_DeepGreen\\Components\\TrainModel.py:129: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if features == lstm_features.size(2):  # If dimensions match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported and saved to Models/Tempus_v2.pt\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T02:02:32.806217Z",
     "start_time": "2025-03-31T02:02:30.553037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def load_and_predict_with_torchscript(script_path, out_of_sample_data, device, window_size):\n",
    "    \"\"\"\n",
    "    Loads a TorchScript model and uses it for prediction on out-of-sample data.\n",
    "\n",
    "    Parameters:\n",
    "        script_path (str): Path to the saved TorchScript model\n",
    "        out_of_sample_data (DataFrame): New data for prediction\n",
    "        device (str): Device on which to run inference\n",
    "        window_size (int): Window size used during training\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predictions from the model\n",
    "    \"\"\"\n",
    "    # Load the TorchScript model\n",
    "    loaded_model = torch.jit.load(script_path)\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "\n",
    "    print(\"TorchScript model loaded successfully\")\n",
    "\n",
    "    # Prepare the out-of-sample data\n",
    "    # Assuming similar preprocessing as in the DataModule class\n",
    "\n",
    "    # If using same format as your DataModule\n",
    "    from Components.TrainModel import DataModule\n",
    "    data_module_test = DataModule(\n",
    "        out_of_sample_data,\n",
    "        window_size=window_size,\n",
    "        batch_size=1,  # For prediction, we can use batch size of 1\n",
    "        eval_size=0.0  # All data is for testing\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_module_test.test_loader:\n",
    "            X = X.to(device)\n",
    "            output = loaded_model(X)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    all_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "# Example usage with out-of-sample data\n",
    "# Assuming you have some out-of-sample data for testing\n",
    "# For this example, I'll use a portion of the test data as \"out-of-sample\"\n",
    "\n",
    "# Get out-of-sample data (this is just an example, replace with your actual out-of-sample data)\n",
    "# One option is to use the most recent data that wasn't used in training\n",
    "ticker = \"IONQ\"  # Replace with your ticker of interest\n",
    "out_of_sample_data, raw_stock_data = TickerData(ticker, years=1, prediction_window=5).process_all()\n",
    "\n",
    "# Load the model and make predictions\n",
    "predictions = load_and_predict_with_torchscript(\n",
    "    script_path=\"Models/Tempus_v2.pt\",\n",
    "    out_of_sample_data=out_of_sample_data,\n",
    "    device=device,\n",
    "    window_size=50\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions for {ticker}\")\n",
    "\n",
    "# Visualize the predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(predictions[:, 0], label='Predicted Price Change (%)')\n",
    "plt.title(f'Price Change Predictions for {ticker} using TorchScript Model')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Price Change (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "67cebfee2dae8fbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchScript model loaded successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (163)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 59\u001B[0m\n\u001B[0;32m     56\u001B[0m out_of_sample_data, raw_stock_data \u001B[38;5;241m=\u001B[39m TickerData(ticker, years\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, prediction_window\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mprocess_all()\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Load the model and make predictions\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m predictions \u001B[38;5;241m=\u001B[39m load_and_predict_with_torchscript(\n\u001B[0;32m     60\u001B[0m     script_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModels/Tempus_v2.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     61\u001B[0m     out_of_sample_data\u001B[38;5;241m=\u001B[39mout_of_sample_data,\n\u001B[0;32m     62\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[0;32m     63\u001B[0m     window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m\n\u001B[0;32m     64\u001B[0m )\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(predictions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m predictions for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mticker\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Visualize the predictions\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 27\u001B[0m, in \u001B[0;36mload_and_predict_with_torchscript\u001B[1;34m(script_path, out_of_sample_data, device, window_size)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Prepare the out-of-sample data\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Assuming similar preprocessing as in the DataModule class\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# If using same format as your DataModule\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mComponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mTrainModel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataModule\n\u001B[1;32m---> 27\u001B[0m data_module_test \u001B[38;5;241m=\u001B[39m DataModule(\n\u001B[0;32m     28\u001B[0m     out_of_sample_data,\n\u001B[0;32m     29\u001B[0m     window_size\u001B[38;5;241m=\u001B[39mwindow_size,\n\u001B[0;32m     30\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,  \u001B[38;5;66;03m# For prediction, we can use batch size of 1\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     eval_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m  \u001B[38;5;66;03m# All data is for testing\u001B[39;00m\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[0;32m     35\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\PycharmProjects\\Project_DeepGreen\\Components\\TrainModel.py:717\u001B[0m, in \u001B[0;36mDataModule.__init__\u001B[1;34m(self, data, window_size, batch_size, eval_size, random_state)\u001B[0m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_size \u001B[38;5;241m=\u001B[39m eval_size\n\u001B[0;32m    715\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state \u001B[38;5;241m=\u001B[39m random_state\n\u001B[1;32m--> 717\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Project_DeepGreen\\Components\\TrainModel.py:747\u001B[0m, in \u001B[0;36mDataModule.setup\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    745\u001B[0m \u001B[38;5;66;03m# Add target column back\u001B[39;00m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_train_scaled[target_col] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_train[target_col]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m--> 747\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_test_scaled[target_col] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_test[target_col]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m    749\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset \u001B[38;5;241m=\u001B[39m SequenceDataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_train_scaled,target\u001B[38;5;241m=\u001B[39mtarget_col,features\u001B[38;5;241m=\u001B[39mfeature_cols,window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwindow_size)\n\u001B[0;32m    750\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[0;32m   4309\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4310\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[1;32m-> 4311\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_item(key, value)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4514\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4515\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4516\u001B[0m \u001B[38;5;124;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[0;32m   4517\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4522\u001B[0m \u001B[38;5;124;03m    ensure homogeneity.\u001B[39;00m\n\u001B[0;32m   4523\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4524\u001B[0m     value, refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sanitize_column(value)\n\u001B[0;32m   4526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   4527\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m   4528\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   4529\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value\u001B[38;5;241m.\u001B[39mdtype, ExtensionDtype)\n\u001B[0;32m   4530\u001B[0m     ):\n\u001B[0;32m   4531\u001B[0m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[0;32m   4532\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns, MultiIndex):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001B[0m, in \u001B[0;36mDataFrame._sanitize_column\u001B[1;34m(self, value)\u001B[0m\n\u001B[0;32m   5263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _reindex_for_setitem(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[0;32m   5265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[1;32m-> 5266\u001B[0m     com\u001B[38;5;241m.\u001B[39mrequire_length_match(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[0;32m   5267\u001B[0m arr \u001B[38;5;241m=\u001B[39m sanitize_array(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, allow_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   5268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5269\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(value, Index)\n\u001B[0;32m   5270\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5273\u001B[0m     \u001B[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001B[39;00m\n\u001B[0;32m   5274\u001B[0m     \u001B[38;5;66;03m# this deprecation\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001B[0m, in \u001B[0;36mrequire_length_match\u001B[1;34m(data, index)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;124;03mCheck the length of data matches the length of the index.\u001B[39;00m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(index):\n\u001B[1;32m--> 573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of values \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    575\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    576\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoes not match length of index \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    577\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(index)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    578\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Length of values (0) does not match length of index (163)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T01:57:02.324684Z",
     "start_time": "2025-03-31T01:57:02.315996Z"
    }
   },
   "cell_type": "code",
   "source": "predictions",
   "id": "e661231e3ba00e56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 197.23183 ]],\n",
       "\n",
       "       [[ 199.64954 ]],\n",
       "\n",
       "       [[ 219.9546  ]],\n",
       "\n",
       "       [[ 245.14612 ]],\n",
       "\n",
       "       [[ 243.0559  ]],\n",
       "\n",
       "       [[ 222.05608 ]],\n",
       "\n",
       "       [[ 204.10439 ]],\n",
       "\n",
       "       [[ 166.6733  ]],\n",
       "\n",
       "       [[ 142.82838 ]],\n",
       "\n",
       "       [[ 103.549255]],\n",
       "\n",
       "       [[ 106.09485 ]],\n",
       "\n",
       "       [[ 112.11592 ]],\n",
       "\n",
       "       [[  65.137634]],\n",
       "\n",
       "       [[  59.818314]],\n",
       "\n",
       "       [[  42.377377]],\n",
       "\n",
       "       [[  67.400314]],\n",
       "\n",
       "       [[  28.211246]],\n",
       "\n",
       "       [[ 335.3945  ]],\n",
       "\n",
       "       [[ 277.64224 ]],\n",
       "\n",
       "       [[ 174.39905 ]],\n",
       "\n",
       "       [[ 372.317   ]],\n",
       "\n",
       "       [[ 456.91788 ]],\n",
       "\n",
       "       [[ 523.6588  ]],\n",
       "\n",
       "       [[ 526.3262  ]],\n",
       "\n",
       "       [[ 662.19684 ]],\n",
       "\n",
       "       [[ 727.921   ]],\n",
       "\n",
       "       [[ 839.13196 ]],\n",
       "\n",
       "       [[1142.3556  ]],\n",
       "\n",
       "       [[1283.5662  ]],\n",
       "\n",
       "       [[1299.2537  ]],\n",
       "\n",
       "       [[1823.0692  ]],\n",
       "\n",
       "       [[2044.4244  ]],\n",
       "\n",
       "       [[2401.3284  ]],\n",
       "\n",
       "       [[2903.4006  ]],\n",
       "\n",
       "       [[3192.3923  ]],\n",
       "\n",
       "       [[3339.8413  ]],\n",
       "\n",
       "       [[3675.4111  ]],\n",
       "\n",
       "       [[4101.0884  ]],\n",
       "\n",
       "       [[4474.4575  ]],\n",
       "\n",
       "       [[4557.9507  ]],\n",
       "\n",
       "       [[4735.2397  ]],\n",
       "\n",
       "       [[4621.1313  ]],\n",
       "\n",
       "       [[4626.386   ]],\n",
       "\n",
       "       [[4628.478   ]],\n",
       "\n",
       "       [[4869.6626  ]],\n",
       "\n",
       "       [[5158.1343  ]],\n",
       "\n",
       "       [[6011.701   ]],\n",
       "\n",
       "       [[6558.5044  ]],\n",
       "\n",
       "       [[6694.221   ]],\n",
       "\n",
       "       [[6830.074   ]],\n",
       "\n",
       "       [[7261.5713  ]],\n",
       "\n",
       "       [[7382.0474  ]],\n",
       "\n",
       "       [[7694.5083  ]],\n",
       "\n",
       "       [[7584.3403  ]],\n",
       "\n",
       "       [[7831.6997  ]],\n",
       "\n",
       "       [[7997.1387  ]],\n",
       "\n",
       "       [[8278.534   ]],\n",
       "\n",
       "       [[8292.644   ]],\n",
       "\n",
       "       [[8306.866   ]],\n",
       "\n",
       "       [[8342.757   ]],\n",
       "\n",
       "       [[8450.692   ]],\n",
       "\n",
       "       [[8684.717   ]],\n",
       "\n",
       "       [[8518.8     ]],\n",
       "\n",
       "       [[8570.284   ]],\n",
       "\n",
       "       [[8627.323   ]],\n",
       "\n",
       "       [[8761.737   ]],\n",
       "\n",
       "       [[8858.522   ]],\n",
       "\n",
       "       [[8729.96    ]],\n",
       "\n",
       "       [[8684.729   ]],\n",
       "\n",
       "       [[8580.489   ]],\n",
       "\n",
       "       [[8599.293   ]],\n",
       "\n",
       "       [[8789.583   ]],\n",
       "\n",
       "       [[9130.353   ]],\n",
       "\n",
       "       [[9207.765   ]],\n",
       "\n",
       "       [[8950.295   ]],\n",
       "\n",
       "       [[8954.969   ]],\n",
       "\n",
       "       [[9229.249   ]],\n",
       "\n",
       "       [[9106.498   ]],\n",
       "\n",
       "       [[9222.08    ]],\n",
       "\n",
       "       [[9353.638   ]],\n",
       "\n",
       "       [[9244.203   ]],\n",
       "\n",
       "       [[9194.821   ]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T00:24:13.316292Z",
     "start_time": "2025-03-31T00:24:13.310470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add trading signals based on predictions\n",
    "training_data['entry_signal'] = np.where(\n",
    "    (training_data['Predicted'].notna()) &\n",
    "    (training_data['Predicted'] > training_data['Close'] * 1.05),  # 5% increase prediction\n",
    "    1,  # Buy signal\n",
    "    0\n",
    ")\n",
    "\n",
    "training_data['exit_signal'] = np.where(\n",
    "    (training_data['Predicted'].notna()) &\n",
    "    (training_data['Predicted'] < training_data['Close'] * 0.95),  # 5% decrease prediction\n",
    "    1,  # Sell signal\n",
    "    0\n",
    ")\n"
   ],
   "id": "3cf01a93de94bd2d",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "ecdb1e7bec75b597",
   "metadata": {},
   "source": [
    "# Get predictions\n",
    "preds_df = model.get_predictions(training_data)\n",
    "merged_df = pd.merge(stock_data, preds_df, on=['Date', 'Ticker'], how='inner')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93f7fb70-590e-42e7-b906-253a5a91d60b",
   "metadata": {},
   "source": [
    "# Create a combined plot with stock prices and prediction markers\n",
    "def plot_combined_predictions(data, ticker):\n",
    "    # Filter for a particular ticker\n",
    "    if type(ticker) == str:\n",
    "        data = data[data['Ticker'] == ticker]\n",
    "    else:\n",
    "        return \"Ticker provided is not a valid value\"\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot stock price trend line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data['Date'],\n",
    "        y=data['Close'],\n",
    "        mode='lines',\n",
    "        name='Stock Price',\n",
    "        line=dict(width=1)\n",
    "    ))\n",
    "\n",
    "    # Split signals by type and correctness\n",
    "    buy_signals = data[data['Predicted'] == 2]\n",
    "    sell_signals = data[data['Predicted'] == 1]\n",
    "    hold_signals = data[data['Predicted'] == 0]\n",
    "\n",
    "    # Correct/incorrect buy signals\n",
    "    correct_buy = buy_signals[buy_signals['Predicted'] == buy_signals['Actual']]\n",
    "    incorrect_buy = buy_signals[buy_signals['Predicted'] != buy_signals['Actual']]\n",
    "\n",
    "    # Correct/incorrect sell signals\n",
    "    correct_sell = sell_signals[sell_signals['Predicted'] == sell_signals['Actual']]\n",
    "    incorrect_sell = sell_signals[sell_signals['Predicted'] != sell_signals['Actual']]\n",
    "\n",
    "    # Correct/incorrect hold signals\n",
    "    correct_hold = hold_signals[hold_signals['Predicted'] == hold_signals['Actual']]\n",
    "    incorrect_hold = hold_signals[hold_signals['Predicted'] != hold_signals['Actual']]\n",
    "\n",
    "    # Plot buy signals\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_buy['Date'],\n",
    "        y=data.loc[correct_buy.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Buy Signal',\n",
    "        marker=dict(symbol='triangle-up', size=10, color='green')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_buy['Date'],\n",
    "        y=data.loc[incorrect_buy.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Buy Signal',\n",
    "        marker=dict(symbol='triangle-up', size=8, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Plot sell signals\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_sell['Date'],\n",
    "        y=data.loc[correct_sell.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Sell Signal',\n",
    "        marker=dict(symbol='triangle-down', size=10, color='red')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_sell['Date'],\n",
    "        y=data.loc[incorrect_sell.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Sell Signal',\n",
    "        marker=dict(symbol='triangle-down', size=8, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Plot hold signals (using a different symbol)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_hold['Date'],\n",
    "        y=data.loc[correct_hold.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Hold Signal',\n",
    "        marker=dict(symbol='circle', size=8, color='blue')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_hold['Date'],\n",
    "        y=data.loc[incorrect_hold.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Hold Signal',\n",
    "        marker=dict(symbol='circle', size=6, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{ticker} Stock Price - Actual/Predicted Signals',\n",
    "        #xaxis_title='Date',\n",
    "        yaxis_title='Price (USD)',\n",
    "        template='plotly_dark',\n",
    "        height=600,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Call the modified function\n",
    "plot_combined_predictions(merged_df, 'PLTR')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "791757bc-255b-416a-8d6b-7fc72b79ba4f",
   "metadata": {},
   "source": [
    "from Components.BackTesting import BackTesting\n",
    "import pandas as pd\n",
    "\n",
    "#merged_df = pd.read_csv('Data/NASDAQ_100_PredictictionsData.csv')\n",
    "\n",
    "initial_capital = 10000.0\n",
    "ticker = 'PLTR'\n",
    "backtester = BackTesting(merged_df, ticker, initial_capital)\n",
    "results, _ = backtester.run_simulation()\n",
    "trades_fig, value_fig, exposure_fig = backtester.plot_performance()\n",
    "trades_fig.show()\n",
    "value_fig.show()\n",
    "exposure_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34fc912d83ccedf8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7871d1ece2811692",
   "metadata": {},
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, dilation, padding, dropout=0.2):\n",
    "        super(TCNBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=output_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu2 = nn.ReLU()  # Added missing relu2 activation\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Residual connection if dimensions don't match\n",
    "        self.residual = nn.Conv1d(input_dim, output_dim, 1) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First conv block\n",
    "        # Residual input\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.relu2(out)  # Correctly use relu2\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        # Return to original shape\n",
    "        # Add the residual and pass through final activation\n",
    "        return self.relu1(out + residual)  # Fixed to use relu1 for the final activation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e373858219796207",
   "metadata": {},
   "source": [
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9,\n",
    "                 sparsity=0.1, noise=0.001, bidirectional=False):\n",
    "        super(EchoStateNetwork, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Input weights (fixed)\n",
    "        self.register_buffer('W_in', self._initialize_input_weights())\n",
    "\n",
    "        # Reservoir weights (fixed)\n",
    "        self.register_buffer('W', self._initialize_reservoir_weights())\n",
    "\n",
    "        # Output weights (trainable)\n",
    "        self.W_out = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            # Second set of weights for backward direction\n",
    "            self.register_buffer('W_in_reverse', self._initialize_input_weights())\n",
    "            self.register_buffer('W_reverse', self._initialize_reservoir_weights())\n",
    "            self.W_out_reverse = nn.Linear(reservoir_size, output_size)\n",
    "            # Combined output\n",
    "            self.W_combined = nn.Linear(output_size * 2, output_size)\n",
    "\n",
    "    def _initialize_input_weights(self):\n",
    "        W_in = torch.zeros(self.reservoir_size, self.input_size)\n",
    "        W_in = torch.nn.init.xavier_uniform_(W_in)\n",
    "        return W_in\n",
    "\n",
    "    def _initialize_reservoir_weights(self):\n",
    "        # Create sparse matrix\n",
    "        W = torch.zeros(self.reservoir_size, self.reservoir_size)\n",
    "        num_connections = int(self.sparsity * self.reservoir_size * self.reservoir_size)\n",
    "        indices = torch.randperm(self.reservoir_size * self.reservoir_size)[:num_connections]\n",
    "        rows = indices // self.reservoir_size\n",
    "        cols = indices % self.reservoir_size\n",
    "        values = torch.randn(num_connections)\n",
    "        W[rows, cols] = values\n",
    "\n",
    "        # Scale to desired spectral radius\n",
    "        eigenvalues = torch.linalg.eigvals(W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        W = W * (self.spectral_radius / max_eigenvalue)\n",
    "        return W\n",
    "\n",
    "    def _reservoir_step(self, x, h_prev, W_in, W):\n",
    "        \"\"\"Execute one step of the reservoir\"\"\"\n",
    "        # h_new = tanh(W_in @ x + W @ h_prev + noise)\n",
    "        h_new = torch.tanh(torch.mm(x, W_in.t()) + torch.mm(h_prev, W.t()) +\n",
    "                           self.noise * torch.randn(h_prev.shape, device=h_prev.device))\n",
    "        return h_new\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (batch_size, seq_len, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Forward pass\n",
    "        h = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_forward = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h = self._reservoir_step(x[:, t], h, self.W_in, self.W)\n",
    "            outputs_forward.append(self.W_out(h))\n",
    "\n",
    "        outputs_forward = torch.stack(outputs_forward, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        if not self.bidirectional:\n",
    "            return outputs_forward\n",
    "\n",
    "        # Backward pass for bidirectional ESN\n",
    "        h_reverse = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_reverse = []\n",
    "\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            h_reverse = self._reservoir_step(x[:, t], h_reverse, self.W_in_reverse, self.W_reverse)\n",
    "            outputs_reverse.insert(0, self.W_out_reverse(h_reverse))\n",
    "\n",
    "        outputs_reverse = torch.stack(outputs_reverse, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        # Combine forward and backward outputs\n",
    "        combined = torch.cat((outputs_forward, outputs_reverse), dim=2)\n",
    "        return self.W_combined(combined)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8b0067a4686aadf",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62964a9774211cfa",
   "metadata": {},
   "source": [
    "#ticker.get_balance_sheet(freq='quarterly')\n",
    "#ticker.get_calendar()\n",
    "#ticker.get_cash_flow(freq='quarterly')\n",
    "#earnings_data = ticker.get_earnings_dates()\n",
    "#income_statement = ticker.get_income_stmt(freq='yearly').T\n",
    "#ticker.get_institutional_holders()\n",
    "#ticker.get_recommendations()\n",
    "#ticker.get_sustainability()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "387ef9179077621f",
   "metadata": {},
   "source": [
    "# define a function to fetch the options data for a given ticker symbol\n",
    "#def fetch_options_data(ticker_symbol):\n",
    "    #ticker = yf.Ticker(ticker_symbol)\n",
    "#    options_dates = ticker.options\n",
    "#    options_data = ticker.option_chain(date='2025-03-21')\n",
    "#    return options_data.calls, options_data.puts\n",
    "##ionq_stock_data = ionq_stock_data.sort_values(by='Date', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
