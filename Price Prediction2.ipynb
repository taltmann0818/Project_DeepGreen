{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T00:56:46.030069Z",
     "start_time": "2025-04-02T00:56:42.952899Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom libraries\n",
    "from Components.TrainModel import DataModule, TEMPUS, torchscript_predict\n",
    "from Components.TickerData import TickerData\n",
    "from Components.BackTesting import BackTesting\n",
    "\n",
    "# Torch ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ae557-72e6-4a76-a93d-78e2381c05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Feature importance with SHAP values and plot\n",
    "#TODO: buy signals become if prediction > current by some delta (~5%). Reverse is sell (decrease by some delta). Senstitvity analysis should be conducted to compare this delta level\n",
    "#TODO: Use quantstats for a HTMl tearsheet\n",
    "#TODO: Add a Echo State Networks (ESN) layer to the model\n",
    "#TODO: randomly sample 50 tickers, run backtest for all of them, and plot. take average sharpe ratio, and other metrics\n",
    "#TODO: PUll Russell 2000 tickers (small cap equities)\n",
    "#TODO: https://www.alternativesoft.com/img/blog/updated/weighted-rank-portfolios.png"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc304bd6da39d466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T00:56:56.174170Z",
     "start_time": "2025-04-02T00:56:55.844586Z"
    }
   },
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "# Clean up the dataframe\n",
    "nasdaq_tickers = tickers.iloc[:, [1]].to_numpy().flatten()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558d55ed6a26fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T01:04:13.662015Z",
     "start_time": "2025-04-01T01:04:13.270972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "# Clean up the dataframe\n",
    "SnP_tickers = tickers.iloc[:, [0]].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834eadee44cddc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T01:04:14.542535Z",
     "start_time": "2025-04-01T01:04:14.538693Z"
    }
   },
   "outputs": [],
   "source": [
    "tickers = np.concatenate((nasdaq_tickers, SnP_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "id": "1da234ad-7e05-4dd8-bc3c-9ea24c392f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T00:57:51.245351Z",
     "start_time": "2025-04-02T00:56:58.328047Z"
    }
   },
   "source": [
    "#tickers = ['IONQ','QBTS','RGTI']\n",
    "training_dfs = []\n",
    "stocks_dfs = []\n",
    "for ticker in nasdaq_tickers:\n",
    "    training_data, raw_stock_data = TickerData(ticker,years=10,prediction_window=5).process_all()\n",
    "    training_dfs.append(training_data)\n",
    "    stocks_dfs.append(raw_stock_data)\n",
    "\n",
    "training_data = pd.concat(training_dfs, ignore_index=False)\n",
    "stock_data = pd.concat(stocks_dfs, ignore_index=False)\n",
    "stock_data"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while processing the data for CCEP\n",
      "Error while merging data for CCEP; error: \"['State'] not in index\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2015-04-06 00:00:00-04:00   75.129997   76.650002   74.750000   76.230003   \n",
       "2015-04-07 00:00:00-04:00   76.070000   76.680000   75.620003   75.650002   \n",
       "2015-04-08 00:00:00-04:00   75.680000   76.349998   75.120003   75.440002   \n",
       "2015-04-09 00:00:00-04:00   75.620003   76.349998   75.209999   76.330002   \n",
       "2015-04-10 00:00:00-04:00   76.300003   76.629997   76.000000   76.529999   \n",
       "...                               ...         ...         ...         ...   \n",
       "2025-03-24 00:00:00-04:00  208.000000  210.179993  206.600006  209.869995   \n",
       "2025-03-25 00:00:00-04:00  211.679993  216.389999  211.199997  215.729996   \n",
       "2025-03-26 00:00:00-04:00  215.250000  216.000000  209.630005  211.550003   \n",
       "2025-03-27 00:00:00-04:00  210.350006  212.000000  205.119995  209.449997   \n",
       "2025-03-28 00:00:00-04:00  208.520004  209.882996  203.503998  207.139999   \n",
       "\n",
       "                            Volume  Dividends  Stock Splits Ticker  \n",
       "Date                                                                \n",
       "2015-04-06 00:00:00-04:00  2556500        0.0           0.0   ADBE  \n",
       "2015-04-07 00:00:00-04:00  2601300        0.0           0.0   ADBE  \n",
       "2015-04-08 00:00:00-04:00  1938500        0.0           0.0   ADBE  \n",
       "2015-04-09 00:00:00-04:00  1556200        0.0           0.0   ADBE  \n",
       "2015-04-10 00:00:00-04:00  1801200        0.0           0.0   ADBE  \n",
       "...                            ...        ...           ...    ...  \n",
       "2025-03-24 00:00:00-04:00  1573800        0.0           0.0     ZS  \n",
       "2025-03-25 00:00:00-04:00  2311600        0.0           0.0     ZS  \n",
       "2025-03-26 00:00:00-04:00  1675700        0.0           0.0     ZS  \n",
       "2025-03-27 00:00:00-04:00  1624800        0.0           0.0     ZS  \n",
       "2025-03-28 00:00:00-04:00  2428400        0.0           0.0     ZS  \n",
       "\n",
       "[235449 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-06 00:00:00-04:00</th>\n",
       "      <td>75.129997</td>\n",
       "      <td>76.650002</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>76.230003</td>\n",
       "      <td>2556500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 00:00:00-04:00</th>\n",
       "      <td>76.070000</td>\n",
       "      <td>76.680000</td>\n",
       "      <td>75.620003</td>\n",
       "      <td>75.650002</td>\n",
       "      <td>2601300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 00:00:00-04:00</th>\n",
       "      <td>75.680000</td>\n",
       "      <td>76.349998</td>\n",
       "      <td>75.120003</td>\n",
       "      <td>75.440002</td>\n",
       "      <td>1938500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-09 00:00:00-04:00</th>\n",
       "      <td>75.620003</td>\n",
       "      <td>76.349998</td>\n",
       "      <td>75.209999</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>1556200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-10 00:00:00-04:00</th>\n",
       "      <td>76.300003</td>\n",
       "      <td>76.629997</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>76.529999</td>\n",
       "      <td>1801200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-24 00:00:00-04:00</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>210.179993</td>\n",
       "      <td>206.600006</td>\n",
       "      <td>209.869995</td>\n",
       "      <td>1573800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-25 00:00:00-04:00</th>\n",
       "      <td>211.679993</td>\n",
       "      <td>216.389999</td>\n",
       "      <td>211.199997</td>\n",
       "      <td>215.729996</td>\n",
       "      <td>2311600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-26 00:00:00-04:00</th>\n",
       "      <td>215.250000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>209.630005</td>\n",
       "      <td>211.550003</td>\n",
       "      <td>1675700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-27 00:00:00-04:00</th>\n",
       "      <td>210.350006</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>205.119995</td>\n",
       "      <td>209.449997</td>\n",
       "      <td>1624800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-28 00:00:00-04:00</th>\n",
       "      <td>208.520004</td>\n",
       "      <td>209.882996</td>\n",
       "      <td>203.503998</td>\n",
       "      <td>207.139999</td>\n",
       "      <td>2428400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ZS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235449 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T02:52:48.500063Z",
     "start_time": "2025-04-02T02:52:48.091244Z"
    }
   },
   "cell_type": "code",
   "source": "import pyodbc",
   "id": "c52db269254b6303",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so, 0x0002): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib\n  Referenced from: <157C2418-FBB4-3F9F-B952-97899DBF4398> /Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so\n  Reason: tried: '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/lib/libodbc.2.dylib' (no such file), '/usr/lib/libodbc.2.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpyodbc\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: dlopen(/Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so, 0x0002): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib\n  Referenced from: <157C2418-FBB4-3F9F-B952-97899DBF4398> /Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so\n  Reason: tried: '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/lib/libodbc.2.dylib' (no such file), '/usr/lib/libodbc.2.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T02:13:17.908333Z",
     "start_time": "2025-04-02T02:13:17.795891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Import your stock_data dataframe into Azure SQL using SQLAlchemy\n",
    "\n",
    "import os\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.engine import URL\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "def upload_stock_data_using_sqlalchemy():\n",
    "    try:\n",
    "        # Get connection string from environment variables\n",
    "        #connection_string = os.environ[\"AZURE_SQL_CONNECTIONSTRING\"]\n",
    "        connection_string = \"Driver={ODBC Driver 18 for SQL Server};Server=tcp:projectdeepgreen.database.windows.net,1433;Database=us_equities;Uid=taltmann;Pwd= Millie5367!;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "\n",
    "        # Create SQLAlchemy engine\n",
    "        # Note: For Azure SQL with token authentication, we'll need to use a different approach\n",
    "        # than a standard connection string\n",
    "\n",
    "        # Option 1: If your connection string is ODBC format:\n",
    "        engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}\")\n",
    "\n",
    "        # Option 2: If you need to use token authentication:\n",
    "        # This would require additional setup similar to your pyodbc approach\n",
    "\n",
    "        # Reset index to make Date a column instead of the index\n",
    "        data_to_upload = stock_data.reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "        # Drop columns not in the table schema (Dividends and Stock Splits)\n",
    "        data_to_upload = data_to_upload[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "        # Upload the dataframe to SQL\n",
    "        # The to_sql method will handle the insert statements for you\n",
    "        data_to_upload.to_sql(\n",
    "            name='nasdaq_1day',\n",
    "            con=engine,\n",
    "            if_exists='append',  # 'replace' if you want to overwrite, 'append' to add to existing\n",
    "            index=False,\n",
    "            chunksize=1000  # Process in batches of 1000 rows\n",
    "        )\n",
    "\n",
    "        print(f\"Successfully uploaded {len(data_to_upload)} records to nasdaq_1day table\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading data: {str(e)}\")\n",
    "\n",
    "\n",
    "# Execute the function\n",
    "upload_stock_data_using_sqlalchemy()\n"
   ],
   "id": "609e2f3b127fb5c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading data: dlopen(/Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so, 0x0002): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib\n",
      "  Referenced from: <157C2418-FBB4-3F9F-B952-97899DBF4398> /Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so\n",
      "  Reason: tried: '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/lib/libodbc.2.dylib' (no such file), '/usr/lib/libodbc.2.dylib' (no such file, not in dyld cache)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T02:26:17.176959Z",
     "start_time": "2025-04-02T02:26:17.154491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pyodbc, struct\n",
    "from azure import identity\n",
    "\n",
    "connection_string = os.environ[\"AZURE_SQL_CONNECTIONSTRING\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_conn():\n",
    "    credential = identity.DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
    "    token_bytes = credential.get_token(\"https://database.windows.net/.default\").token.encode(\"UTF-16-LE\")\n",
    "    token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
    "    SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by microsoft in msodbcsql.h\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    return conn\n",
    "\n",
    "# %%\n",
    "# Import your stock_data dataframe into Azure SQL\n",
    "\n",
    "def upload_stock_data_to_azure():\n",
    "    # Reset index to make Date a column instead of the index\n",
    "    data_to_upload = stock_data.reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "    # Drop columns not in the table schema (Dividends and Stock Splits)\n",
    "    data_to_upload = data_to_upload[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "    # Connect to Azure SQL\n",
    "    try:\n",
    "        with get_conn() as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Use fast_executemany for better performance with large datasets\n",
    "            cursor.fast_executemany = True\n",
    "\n",
    "            # Prepare the SQL INSERT statement\n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO nasdaq_1day (Date, Ticker, [Open], High, Low, [Close], Volume)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "\n",
    "            # Convert dataframe to list of tuples\n",
    "            records = list(data_to_upload.itertuples(index=False, name=None))\n",
    "\n",
    "            # Execute the insert\n",
    "            cursor.executemany(insert_sql, records)\n",
    "            conn.commit()\n",
    "\n",
    "            print(f\"Successfully uploaded {len(data_to_upload)} records to nasdaq_1day table\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading data: {str(e)}\")\n",
    "\n",
    "\n",
    "# Execute the function\n",
    "upload_stock_data_to_azure()\n"
   ],
   "id": "896040b0970373cb",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so, 0x0002): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib\n  Referenced from: <E38C7B25-7727-3975-BBA6-24065FE028D6> /Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so\n  Reason: tried: '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/lib/libodbc.2.dylib' (no such file), '/usr/lib/libodbc.2.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpyodbc\u001B[39;00m\u001B[38;5;241m,\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mstruct\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m identity\n\u001B[1;32m      5\u001B[0m connection_string \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAZURE_SQL_CONNECTIONSTRING\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mImportError\u001B[0m: dlopen(/Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so, 0x0002): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib\n  Referenced from: <E38C7B25-7727-3975-BBA6-24065FE028D6> /Users/thomasaltmann/opt/anaconda3/lib/python3.8/site-packages/pyodbc.cpython-38-darwin.so\n  Reason: tried: '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/opt/unixodbc/lib/libodbc.2.dylib' (no such file), '/usr/local/lib/libodbc.2.dylib' (no such file), '/usr/lib/libodbc.2.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea3a893a24f1b0",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c08f8808767bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:14:12.931755Z",
     "start_time": "2025-04-01T01:10:04.658281Z"
    }
   },
   "outputs": [],
   "source": [
    "#Best config: {'lr': 4.390449033248878e-05, 'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3477694988633191, 'weight_decay': 0.0001801390872725824, 'batch_size': 16, 'window_size': 10, 'grad_clip_norm': 0.8393802881451728}\n",
    "\n",
    "config = {\n",
    "    \"lr\": 4.390449033248878e-05,\n",
    "    \"weight_decay\": 0.0001801390872725824,\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.3477694988633191,\n",
    "    \"batch_size\": 16,\n",
    "    \"window_size\": 50,\n",
    "    \"clip_size\": 0.8393802881451728,\n",
    "    \"epochs\": 20,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "data_module = DataModule(training_data, window_size=config[\"window_size\"], batch_size=config[\"batch_size\"])\n",
    "config[\"input_size\"] = data_module.num_features\n",
    "\n",
    "# Instantiate the model\n",
    "model = TEMPUS(config,scaler=data_module.scaler)\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "# Train Model\n",
    "history = model.train_model(data_module.train_loader, data_module.test_loader, criterion, optimizer, config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810ba6b-76da-4ada-9f44-e7302062a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fig = model.plot_training_history()\n",
    "training_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8c392578867a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:17:48.756451Z",
     "start_time": "2025-04-01T02:17:48.270108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export the trained TEMPUS model\n",
    "script_path = model.export_model_to_torchscript(\n",
    "    save_path=\"Models/Tempus_v2.pt\",\n",
    "    data_loader=data_module.test_loader,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cebfee2dae8fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:19:41.215572Z",
     "start_time": "2025-04-01T02:19:39.489452Z"
    }
   },
   "outputs": [],
   "source": [
    "ticker = \"PLTR\"  # Replace with your ticker of interest\n",
    "out_of_sample_data, raw_stock_data = TickerData(ticker, years=1, prediction_window=5).process_all()\n",
    "\n",
    "# Load the model and make predictions\n",
    "preds_df = torchscript_predict(\n",
    "    model_path=\"Models/Tempus_v2.pt\",\n",
    "    input_df=out_of_sample_data,\n",
    "    device=\"cpu\",\n",
    "    window_size=50,\n",
    "    target_col=\"shifted_prices\"\n",
    ")\n",
    "\n",
    "preds_df = pd.merge(preds_df, raw_stock_data[['Open', 'High', 'Low', 'Volume','Close']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=preds_df['Predicted'], x=preds_df.index, mode='lines', name='Predicted',line=dict(color=\"Grey\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['Close'], x=preds_df.index, mode='lines', name='Close (Unshifted)',line=dict(color=\"Blue\")))\n",
    "fig.add_trace(go.Scatter(y=preds_df['Actual'], x=preds_df.index, mode='lines', name='Close (Shifted)'))\n",
    "fig.update_layout(title=f'Prediction for {ticker}', xaxis_title='Date', yaxis_title='Price (USD)',height=600,legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02))\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d63aa-1615-4e8e-8d7d-b3199ed03f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T02:19:47.087751Z",
     "start_time": "2025-04-01T02:19:47.075827Z"
    }
   },
   "outputs": [],
   "source": [
    "from Components.BackTesting import BackTesting\n",
    "import pandas as pd\n",
    "\n",
    "initial_capital = 1000.0\n",
    "ticker = 'PLTR'\n",
    "backtester = BackTesting(preds_df, ticker, initial_capital,pct_change_entry=0.05,pct_change_exit=0.02)\n",
    "results, _ = backtester.run_simulation()\n",
    "trades_fig, value_fig, exposure_fig = backtester.plot_performance()\n",
    "#trades_fig.show()\n",
    "#value_fig.show()\n",
    "#exposure_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf01a93de94bd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb1e7bec75b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "returns = backtester.pf.returns()\n",
    "\n",
    "#html = qs.reports.full(returns, \"NDAQ\")\n",
    "pd.DataFrame(qs.reports.metrics(returns, \"NDAQ\",mode='full',rf=0.0025, display=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7fb70-590e-42e7-b906-253a5a91d60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730ed61-badd-4f39-acb0-057ed8547250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791757bc-255b-416a-8d6b-7fc72b79ba4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc912d83ccedf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871d1ece2811692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, dilation, padding, dropout=0.2):\n",
    "        super(TCNBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=output_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu2 = nn.ReLU()  # Added missing relu2 activation\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Residual connection if dimensions don't match\n",
    "        self.residual = nn.Conv1d(input_dim, output_dim, 1) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First conv block\n",
    "        # Residual input\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.relu2(out)  # Correctly use relu2\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        # Return to original shape\n",
    "        # Add the residual and pass through final activation\n",
    "        return self.relu1(out + residual)  # Fixed to use relu1 for the final activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373858219796207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9,\n",
    "                 sparsity=0.1, noise=0.001, bidirectional=False):\n",
    "        super(EchoStateNetwork, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Input weights (fixed)\n",
    "        self.register_buffer('W_in', self._initialize_input_weights())\n",
    "\n",
    "        # Reservoir weights (fixed)\n",
    "        self.register_buffer('W', self._initialize_reservoir_weights())\n",
    "\n",
    "        # Output weights (trainable)\n",
    "        self.W_out = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            # Second set of weights for backward direction\n",
    "            self.register_buffer('W_in_reverse', self._initialize_input_weights())\n",
    "            self.register_buffer('W_reverse', self._initialize_reservoir_weights())\n",
    "            self.W_out_reverse = nn.Linear(reservoir_size, output_size)\n",
    "            # Combined output\n",
    "            self.W_combined = nn.Linear(output_size * 2, output_size)\n",
    "\n",
    "    def _initialize_input_weights(self):\n",
    "        W_in = torch.zeros(self.reservoir_size, self.input_size)\n",
    "        W_in = torch.nn.init.xavier_uniform_(W_in)\n",
    "        return W_in\n",
    "\n",
    "    def _initialize_reservoir_weights(self):\n",
    "        # Create sparse matrix\n",
    "        W = torch.zeros(self.reservoir_size, self.reservoir_size)\n",
    "        num_connections = int(self.sparsity * self.reservoir_size * self.reservoir_size)\n",
    "        indices = torch.randperm(self.reservoir_size * self.reservoir_size)[:num_connections]\n",
    "        rows = indices // self.reservoir_size\n",
    "        cols = indices % self.reservoir_size\n",
    "        values = torch.randn(num_connections)\n",
    "        W[rows, cols] = values\n",
    "\n",
    "        # Scale to desired spectral radius\n",
    "        eigenvalues = torch.linalg.eigvals(W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        W = W * (self.spectral_radius / max_eigenvalue)\n",
    "        return W\n",
    "\n",
    "    def _reservoir_step(self, x, h_prev, W_in, W):\n",
    "        \"\"\"Execute one step of the reservoir\"\"\"\n",
    "        # h_new = tanh(W_in @ x + W @ h_prev + noise)\n",
    "        h_new = torch.tanh(torch.mm(x, W_in.t()) + torch.mm(h_prev, W.t()) +\n",
    "                           self.noise * torch.randn(h_prev.shape, device=h_prev.device))\n",
    "        return h_new\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (batch_size, seq_len, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Forward pass\n",
    "        h = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_forward = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h = self._reservoir_step(x[:, t], h, self.W_in, self.W)\n",
    "            outputs_forward.append(self.W_out(h))\n",
    "\n",
    "        outputs_forward = torch.stack(outputs_forward, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        if not self.bidirectional:\n",
    "            return outputs_forward\n",
    "\n",
    "        # Backward pass for bidirectional ESN\n",
    "        h_reverse = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_reverse = []\n",
    "\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            h_reverse = self._reservoir_step(x[:, t], h_reverse, self.W_in_reverse, self.W_reverse)\n",
    "            outputs_reverse.insert(0, self.W_out_reverse(h_reverse))\n",
    "\n",
    "        outputs_reverse = torch.stack(outputs_reverse, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        # Combine forward and backward outputs\n",
    "        combined = torch.cat((outputs_forward, outputs_reverse), dim=2)\n",
    "        return self.W_combined(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0067a4686aadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62964a9774211cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ticker.get_balance_sheet(freq='quarterly')\n",
    "#ticker.get_calendar()\n",
    "#ticker.get_cash_flow(freq='quarterly')\n",
    "#earnings_data = ticker.get_earnings_dates()\n",
    "#income_statement = ticker.get_income_stmt(freq='yearly').T\n",
    "#ticker.get_institutional_holders()\n",
    "#ticker.get_recommendations()\n",
    "#ticker.get_sustainability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ef9179077621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to fetch the options data for a given ticker symbol\n",
    "#def fetch_options_data(ticker_symbol):\n",
    "    #ticker = yf.Ticker(ticker_symbol)\n",
    "#    options_dates = ticker.options\n",
    "#    options_data = ticker.option_chain(date='2025-03-21')\n",
    "#    return options_data.calls, options_data.puts\n",
    "##ionq_stock_data = ionq_stock_data.sort_values(by='Date', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
