{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T01:47:33.994793Z",
     "start_time": "2025-03-29T01:47:29.175555Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Custom libraries\n",
    "from Components.TrainModel import DataModule#, TEMPUS\n",
    "from Components.TickerData import TickerData\n",
    "from Components.BackTesting import BackTesting\n",
    "\n",
    "# Torch ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b00ae557-72e6-4a76-a93d-78e2381c05e0",
   "metadata": {},
   "source": [
    "#TODO: Feature importance with SHAP values and plot\n",
    "#TODO: hyperparameter tuning\n",
    "#TODO: buy signals become if prediction > current by some delta (~5%). Reverse is sell (decrease by some delta). Senstitvity analysis should be conducted to compare this delta level\n",
    "#TODO: Use quantstats for a HTMl tearsheet\n",
    "#TODO: market-regime detector with Hiden-markov model\n",
    "#TODO: Add a Echo State Networks (ESN) layer to the model\n",
    "#TODO: randomly sample 50 tickers, run backtest for all of them, and plot. take average sharpe ratio, and other metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc304bd6da39d466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:45:39.874314Z",
     "start_time": "2025-03-29T00:45:39.520095Z"
    }
   },
   "source": [
    "# Set the Wikipedia page title and section header\n",
    "tickers = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "# Clean up the dataframe\n",
    "tickers = tickers.iloc[:, [1]].to_numpy().flatten()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1da234ad-7e05-4dd8-bc3c-9ea24c392f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:46:32.067599Z",
     "start_time": "2025-03-29T00:45:41.365646Z"
    }
   },
   "source": [
    "#tickers = ['IONQ','QBTS','RGTI']\n",
    "training_dfs = []\n",
    "stocks_dfs = []\n",
    "for ticker in tickers:\n",
    "    training_data, raw_stock_data = TickerData(ticker,years=10).process_all()\n",
    "    training_dfs.append(training_data)\n",
    "    stocks_dfs.append(raw_stock_data)\n",
    "\n",
    "training_data = pd.concat(training_dfs, ignore_index=False)\n",
    "stock_data = pd.concat(stocks_dfs, ignore_index=False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3d3bf5fc-fc97-42e8-b637-bb2dea141680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T00:47:52.933082Z",
     "start_time": "2025-03-29T00:47:49.715082Z"
    }
   },
   "source": [
    "#training_data.to_csv(\"Data/NASDAQ_100_TrainingData_v2.csv\", index=True)\n",
    "#stock_data.to_csv(\"Data/NASDAQ_100_StockData_v2.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "f39a8f78-fe7d-41e9-8e51-0a522e986489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T01:47:36.523555Z",
     "start_time": "2025-03-29T01:47:36.268739Z"
    }
   },
   "source": [
    "training_data = pd.read_csv(\"Data/NASDAQ_100_TrainingData_v2.csv\")\n",
    "training_data = training_data.set_index(training_data['Date']).drop(columns=['Date'])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e96a1096-cd23-467c-befe-284716d6b4d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T01:47:37.440814Z",
     "start_time": "2025-03-29T01:47:37.232951Z"
    }
   },
   "source": [
    "stock_data = pd.read_csv(\"Data/NASDAQ_100_StockData_v2.csv\")\n",
    "stock_data = stock_data.set_index(stock_data['Date']).drop(columns=['Date'])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c7d1305cef4d7a72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T01:48:04.016206Z",
     "start_time": "2025-03-29T01:48:03.980789Z"
    }
   },
   "source": "data_module = DataModule(training_data, window_size=50, batch_size=32)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T02:59:11.725183Z",
     "start_time": "2025-03-29T02:59:11.709751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import shap\n",
    "\n",
    "class TEMPUS(nn.Module):\n",
    "    \"\"\"\n",
    "    The TEMPUS model for time-series data processing and prediction tasks.\n",
    "\n",
    "    TEMPUS implements a hybrid deep learning architecture\n",
    "    that combines LSTM at multiple temporal resolutions, Temporal Convolution Networks (TCN), feature fusion, Temporal Attention (TA), and fully connected layers for regression tasks. It is designed\n",
    "    to handle sequential data effectively by capturing temporal\n",
    "    relationships and high-level data representations.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2,tcn_kernel_sizes=[3, 5, 7], **kwargs):\n",
    "        super(TEMPUS, self).__init__()\n",
    "        self.device = kwargs.get(device, 'cpu')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # Multiple Temporal Resolutions of LSTM\n",
    "        self.lstm_short = nn.LSTM(input_size=self.input_size,hidden_size=self.hidden_size,num_layers=self.num_layers,batch_first=True, dropout=self.dropout_rate if self.num_layers > 1 else 0,bidirectional=True)\n",
    "        self.lstm_medium = nn.LSTM(input_size=self.input_size,hidden_size=self.hidden_size,num_layers=self.num_layers,batch_first=True,dropout=self.dropout_rate if self.num_layers > 1 else 0,bidirectional=True)\n",
    "        self.temporal_fusion = nn.Linear(hidden_size * 4, hidden_size * 2) # Fusion layer for temporal resolutions\n",
    "\n",
    "        # Temporal Convolutional Network (TCN)\n",
    "        self.tcn_modules = nn.ModuleList()\n",
    "        for k_size in tcn_kernel_sizes:\n",
    "            padding = (k_size - 1) // 2  # Same padding\n",
    "            self.tcn_modules.append(nn.Sequential(\n",
    "                nn.Conv1d(input_size, hidden_size, kernel_size=k_size, padding=padding),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_size, hidden_size, kernel_size=k_size, padding=padding),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        self.tcn_fusion = nn.Linear(hidden_size * len(tcn_kernel_sizes), hidden_size * 2)\n",
    "\n",
    "        # Combine TCN and LSTM features\n",
    "        self.feature_fusion = nn.Linear(hidden_size * 4, hidden_size * 2)\n",
    "\n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(\n",
    "            d_model=hidden_size * 2,\n",
    "            nhead=4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.regression_head = nn.Linear(hidden_size // 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def downsample_sequence(self, x, factor):\n",
    "        \"\"\"Downsample time sequence by average pooling\"\"\"\n",
    "        batch_size, seq_len, features = x.size()\n",
    "        if seq_len % factor != 0:\n",
    "            # Pad sequence if needed\n",
    "            pad_len = factor - (seq_len % factor)\n",
    "            x = F.pad(x, (0, 0, 0, pad_len))\n",
    "            seq_len += pad_len\n",
    "\n",
    "        # Reshape for pooling\n",
    "        x = x.view(batch_size, seq_len // factor, factor, features)\n",
    "        # Average pool\n",
    "        x = torch.mean(x, dim=2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.size()\n",
    "\n",
    "        # Process with TCN\n",
    "        tcn_outputs = []\n",
    "        x_tcn = x.transpose(1, 2)  # TCN expects (batch, channels, seq_len)\n",
    "        for tcn_module in self.tcn_modules:\n",
    "            tcn_out = tcn_module(x_tcn)\n",
    "            tcn_outputs.append(tcn_out)\n",
    "\n",
    "        # Concatenate TCN outputs\n",
    "        tcn_combined = torch.cat(tcn_outputs, dim=1)\n",
    "        tcn_combined = tcn_combined.transpose(1, 2)  # Back to (batch, seq, features)\n",
    "        tcn_features = self.tcn_fusion(tcn_combined)\n",
    "\n",
    "        # Multiple Temporal Resolutions\n",
    "        # Original sequence for short-term patterns\n",
    "        lstm_short_out, _ = self.lstm_short(x)\n",
    "\n",
    "        # Downsampled sequence for medium-term patterns (every 2 time steps)\n",
    "        x_medium = self.downsample_sequence(x, 2)\n",
    "        lstm_medium_out, _ = self.lstm_medium(x_medium)\n",
    "\n",
    "        # Upsample medium resolution back to original sequence length\n",
    "        lstm_medium_out = F.interpolate(\n",
    "            lstm_medium_out.transpose(1, 2),\n",
    "            size=seq_len,\n",
    "            mode='linear'\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # Combine temporal resolutions\n",
    "        lstm_combined = torch.cat([lstm_short_out, lstm_medium_out], dim=2)\n",
    "        lstm_features = self.temporal_fusion(lstm_combined)\n",
    "\n",
    "        # 2. Add residual connection\n",
    "        if features == lstm_features.size(2):  # If dimensions match\n",
    "            lstm_features = lstm_features + x\n",
    "\n",
    "        # Combine LSTM and TCN features\n",
    "        combined_features = torch.cat([lstm_features, tcn_features], dim=2)\n",
    "        fused_features = self.feature_fusion(combined_features)\n",
    "\n",
    "        # Apply temporal attention\n",
    "        attended_features = self.temporal_attention(fused_features)\n",
    "\n",
    "        # Global pooling (average) across time dimension to get single feature vector per sequence\n",
    "        pooled_features = torch.mean(attended_features, dim=1)\n",
    "\n",
    "        # Final output layers\n",
    "        x = F.relu(self.fc1(pooled_features))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        outputs = self.regression_head(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def train_model(self, train_loader, test_loader, criterion, optimizer, num_epochs=10, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Train the model with a regression task\n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "\n",
    "        best_test_mape = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        history = {\n",
    "            'train_loss': [], 'test_loss': [],\n",
    "            'rmse': [], 'mape': []\n",
    "        }\n",
    "\n",
    "        epoch_progress = tqdm(range(num_epochs), desc=\"Training Epochs\")\n",
    "        for epoch in epoch_progress:\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), max_norm=clip_value)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            test_loss, test_rmse, test_mape = self.evaluate(test_loader, criterion)\n",
    "            epoch_progress.set_postfix({\n",
    "                'Train Loss': f'{train_loss:.4f}',\n",
    "                'Test Loss': f'{test_loss:.4f}',\n",
    "                'RMSE': f'{test_rmse:.4f}',\n",
    "                'MAPE': f'{test_mape:.2f}%'\n",
    "            })\n",
    "            #print(f\"Epoch {epoch}/{num_epochs}; Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, RMSE: {test_rmse:.2f}%\")\n",
    "\n",
    "            # Store history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['rmse'].append(test_rmse)\n",
    "            history['mape'].append(test_mape)\n",
    "\n",
    "            # Store model state with best val mape\n",
    "            if test_mape < best_test_mape:\n",
    "                best_test_mape = test_mape\n",
    "                best_model_state = self.state_dict()\n",
    "\n",
    "        # Load the best model state before returning\n",
    "        if best_model_state is not None:\n",
    "            self.load_state_dict(best_model_state)\n",
    "\n",
    "        # Final evaluation with best model state\n",
    "        test_loss, test_rmse, test_mape = self.evaluate(test_loader, criterion)\n",
    "        print(f\"\\nBest Loss: {test_loss:.4f}, Best RMSE: {test_rmse:.4f}, Best MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "        self.history = history\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, data_loader, criterion):\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                all_predictions.append(outputs.to(self.device).numpy())\n",
    "                all_targets.append(targets.to(self.device).numpy())\n",
    "\n",
    "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "        # Calculate RMSE and MAPE\n",
    "        mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        # Avoid division by zero by adding a small epsilon\n",
    "        mape = np.mean(np.abs((all_targets - all_predictions) / (all_targets + 1e-8))) * 100\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "\n",
    "        return avg_loss, rmse, mape"
   ],
   "id": "5660fc666846b107",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T02:26:46.394106Z",
     "start_time": "2025-03-29T02:26:46.388915Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 34,
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, dilation, padding, dropout=0.2):\n",
    "        super(TCNBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=output_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu2 = nn.ReLU()  # Added missing relu2 activation\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Residual connection if dimensions don't match\n",
    "        self.residual = nn.Conv1d(input_dim, output_dim, 1) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First conv block\n",
    "        # Residual input\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.relu2(out)  # Correctly use relu2\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        # Return to original shape\n",
    "        # Add the residual and pass through final activation\n",
    "        return self.relu1(out + residual)  # Fixed to use relu1 for the final activation"
   ],
   "id": "71ce77b40fc54872"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T01:47:51.233714Z",
     "start_time": "2025-03-29T01:47:51.224980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EchoStateNetwork(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9,\n",
    "                 sparsity=0.1, noise=0.001, bidirectional=False):\n",
    "        super(EchoStateNetwork, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Input weights (fixed)\n",
    "        self.register_buffer('W_in', self._initialize_input_weights())\n",
    "\n",
    "        # Reservoir weights (fixed)\n",
    "        self.register_buffer('W', self._initialize_reservoir_weights())\n",
    "\n",
    "        # Output weights (trainable)\n",
    "        self.W_out = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        if bidirectional:\n",
    "            # Second set of weights for backward direction\n",
    "            self.register_buffer('W_in_reverse', self._initialize_input_weights())\n",
    "            self.register_buffer('W_reverse', self._initialize_reservoir_weights())\n",
    "            self.W_out_reverse = nn.Linear(reservoir_size, output_size)\n",
    "            # Combined output\n",
    "            self.W_combined = nn.Linear(output_size * 2, output_size)\n",
    "\n",
    "    def _initialize_input_weights(self):\n",
    "        W_in = torch.zeros(self.reservoir_size, self.input_size)\n",
    "        W_in = torch.nn.init.xavier_uniform_(W_in)\n",
    "        return W_in\n",
    "\n",
    "    def _initialize_reservoir_weights(self):\n",
    "        # Create sparse matrix\n",
    "        W = torch.zeros(self.reservoir_size, self.reservoir_size)\n",
    "        num_connections = int(self.sparsity * self.reservoir_size * self.reservoir_size)\n",
    "        indices = torch.randperm(self.reservoir_size * self.reservoir_size)[:num_connections]\n",
    "        rows = indices // self.reservoir_size\n",
    "        cols = indices % self.reservoir_size\n",
    "        values = torch.randn(num_connections)\n",
    "        W[rows, cols] = values\n",
    "\n",
    "        # Scale to desired spectral radius\n",
    "        eigenvalues = torch.linalg.eigvals(W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        W = W * (self.spectral_radius / max_eigenvalue)\n",
    "        return W\n",
    "\n",
    "    def _reservoir_step(self, x, h_prev, W_in, W):\n",
    "        \"\"\"Execute one step of the reservoir\"\"\"\n",
    "        # h_new = tanh(W_in @ x + W @ h_prev + noise)\n",
    "        h_new = torch.tanh(torch.mm(x, W_in.t()) + torch.mm(h_prev, W.t()) +\n",
    "                           self.noise * torch.randn(h_prev.shape, device=h_prev.device))\n",
    "        return h_new\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (batch_size, seq_len, input_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Forward pass\n",
    "        h = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_forward = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h = self._reservoir_step(x[:, t], h, self.W_in, self.W)\n",
    "            outputs_forward.append(self.W_out(h))\n",
    "\n",
    "        outputs_forward = torch.stack(outputs_forward, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        if not self.bidirectional:\n",
    "            return outputs_forward\n",
    "\n",
    "        # Backward pass for bidirectional ESN\n",
    "        h_reverse = torch.zeros(batch_size, self.reservoir_size, device=x.device)\n",
    "        outputs_reverse = []\n",
    "\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            h_reverse = self._reservoir_step(x[:, t], h_reverse, self.W_in_reverse, self.W_reverse)\n",
    "            outputs_reverse.insert(0, self.W_out_reverse(h_reverse))\n",
    "\n",
    "        outputs_reverse = torch.stack(outputs_reverse, dim=1)  # (batch_size, seq_len, output_size)\n",
    "\n",
    "        # Combine forward and backward outputs\n",
    "        combined = torch.cat((outputs_forward, outputs_reverse), dim=2)\n",
    "        return self.W_combined(combined)"
   ],
   "id": "2f69381802babe0c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class OldTemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.time_attn = nn.Sequential(\n",
    "            nn.Linear(1, 16),  # Simple time feature processing\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        self.feature_attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_output, time_features):\n",
    "        # lstm_output: [batch, seq_len, hidden]\n",
    "        # time_features: [batch, seq_len, 1] - normalized position in sequence\n",
    "\n",
    "        # Compute base attention scores from features\n",
    "        feature_scores = self.feature_attn(lstm_output)  # [batch, seq_len, 1]\n",
    "        # Compute time-based attention\n",
    "        time_weights = self.time_attn(time_features)  # [batch, seq_len, 1]\n",
    "        # Combine feature and time attention\n",
    "        combined_scores = feature_scores + time_weights\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(combined_scores, dim=1)\n",
    "        # Apply attention to get context vector\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), lstm_output)  # [batch, 1, hidden]\n",
    "\n",
    "        return context, attention_weights"
   ],
   "id": "b1a2deabd4a1101f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T02:58:39.787537Z",
     "start_time": "2025-03-29T02:58:39.784301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4, dropout=0.1):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        out = self.norm(x + self.dropout(attn_output))\n",
    "        return out"
   ],
   "id": "d274d3a799ed3b76",
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "788c08f8808767bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T03:16:17.297853Z",
     "start_time": "2025-03-29T02:59:13.672996Z"
    }
   },
   "source": [
    "# Automatically get the number of features given my data_module object\n",
    "input_size = data_module.num_features\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the model\n",
    "model = TEMPUS(input_size, hidden_size, num_layers, dropout)\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# Train Model\n",
    "history = model.train_model(data_module.train_loader, data_module.test_loader, criterion, optimizer, epochs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45b0b67a25324994aeda266cfb2d764e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-44-9229ce10cc8e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAdamW\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight_decay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;31m# Train Model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mhistory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-43-ac52de1c4d66>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(self, train_loader, test_loader, criterion, optimizer, num_epochs, clip_value)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m                 \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m                 \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m                 \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclip_grad_norm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mclip_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m                 \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    521\u001B[0m             )\n\u001B[0;32m--> 522\u001B[0;31m         torch.autograd.backward(\n\u001B[0m\u001B[1;32m    523\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m         )\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 266\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    267\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save_model(\"Models/TEMPUS_NASDAQ_100_v2.pt\")\n",
    "preds_def = model.get_predictions(training_data)\n",
    "preds_def"
   ],
   "id": "ad02c891af565130",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "919aad42-5ba2-419a-b282-199fcbc5c682",
   "metadata": {},
   "source": [
    "# Plot training metrics\n",
    "training_fig = model.plot_training_history(training_data)\n",
    "training_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecdb1e7bec75b597",
   "metadata": {},
   "source": [
    "# Get predictions\n",
    "preds_df = model.get_predictions(training_data)\n",
    "merged_df = pd.merge(stock_data, preds_df, on=['Date', 'Ticker'], how='inner')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93f7fb70-590e-42e7-b906-253a5a91d60b",
   "metadata": {},
   "source": [
    "# Create a combined plot with stock prices and prediction markers\n",
    "def plot_combined_predictions(data, ticker):\n",
    "    # Filter for a particular ticker\n",
    "    if type(ticker) == str:\n",
    "        data = data[data['Ticker'] == ticker]\n",
    "    else:\n",
    "        return \"Ticker provided is not a valid value\"\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot stock price trend line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data['Date'],\n",
    "        y=data['Close'],\n",
    "        mode='lines',\n",
    "        name='Stock Price',\n",
    "        line=dict(width=1)\n",
    "    ))\n",
    "\n",
    "    # Split signals by type and correctness\n",
    "    buy_signals = data[data['Predicted'] == 2]\n",
    "    sell_signals = data[data['Predicted'] == 1]\n",
    "    hold_signals = data[data['Predicted'] == 0]\n",
    "\n",
    "    # Correct/incorrect buy signals\n",
    "    correct_buy = buy_signals[buy_signals['Predicted'] == buy_signals['Actual']]\n",
    "    incorrect_buy = buy_signals[buy_signals['Predicted'] != buy_signals['Actual']]\n",
    "\n",
    "    # Correct/incorrect sell signals\n",
    "    correct_sell = sell_signals[sell_signals['Predicted'] == sell_signals['Actual']]\n",
    "    incorrect_sell = sell_signals[sell_signals['Predicted'] != sell_signals['Actual']]\n",
    "\n",
    "    # Correct/incorrect hold signals\n",
    "    correct_hold = hold_signals[hold_signals['Predicted'] == hold_signals['Actual']]\n",
    "    incorrect_hold = hold_signals[hold_signals['Predicted'] != hold_signals['Actual']]\n",
    "\n",
    "    # Plot buy signals\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_buy['Date'],\n",
    "        y=data.loc[correct_buy.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Buy Signal',\n",
    "        marker=dict(symbol='triangle-up', size=10, color='green')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_buy['Date'],\n",
    "        y=data.loc[incorrect_buy.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Buy Signal',\n",
    "        marker=dict(symbol='triangle-up', size=8, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Plot sell signals\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_sell['Date'],\n",
    "        y=data.loc[correct_sell.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Sell Signal',\n",
    "        marker=dict(symbol='triangle-down', size=10, color='red')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_sell['Date'],\n",
    "        y=data.loc[incorrect_sell.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Sell Signal',\n",
    "        marker=dict(symbol='triangle-down', size=8, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Plot hold signals (using a different symbol)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=correct_hold['Date'],\n",
    "        y=data.loc[correct_hold.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Correct Hold Signal',\n",
    "        marker=dict(symbol='circle', size=8, color='blue')\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=incorrect_hold['Date'],\n",
    "        y=data.loc[incorrect_hold.index]['Close'],\n",
    "        mode='markers',\n",
    "        name='Incorrect Hold Signal',\n",
    "        marker=dict(symbol='circle', size=6, color='gray', opacity=0.2)\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{ticker} Stock Price - Actual/Predicted Signals',\n",
    "        #xaxis_title='Date',\n",
    "        yaxis_title='Price (USD)',\n",
    "        template='plotly_dark',\n",
    "        height=600,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Call the modified function\n",
    "plot_combined_predictions(merged_df, 'PLTR')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "791757bc-255b-416a-8d6b-7fc72b79ba4f",
   "metadata": {},
   "source": [
    "from Components.BackTesting import BackTesting\n",
    "import pandas as pd\n",
    "\n",
    "#merged_df = pd.read_csv('Data/NASDAQ_100_PredictictionsData.csv')\n",
    "\n",
    "initial_capital = 10000.0\n",
    "ticker = 'PLTR'\n",
    "backtester = BackTesting(merged_df, ticker, initial_capital)\n",
    "results, _ = backtester.run_simulation()\n",
    "trades_fig, value_fig, exposure_fig = backtester.plot_performance()\n",
    "trades_fig.show()\n",
    "value_fig.show()\n",
    "exposure_fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34fc912d83ccedf8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7871d1ece2811692",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e373858219796207",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "589e12c4a43e47c6",
   "metadata": {},
   "source": [
    "# %%\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "#from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from Components.TrainModel import TunableLSTMClassifier\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Define a training function for Ray Tune\n",
    "def train_lstm(config, input_size=33, num_classes=3, train_data=None, val_data=None, test_data=None):\n",
    "    # Set up device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # Create model with the hyperparameter configuration\n",
    "    model = TunableLSTMClassifier({\n",
    "        \"input_size\": input_size,\n",
    "        \"hidden_size\": config[\"hidden_size\"],\n",
    "        \"num_layers\": config[\"num_layers\"],\n",
    "        \"num_classes\": num_classes,\n",
    "        \"dropout_rate\": config[\"dropout_rate\"]\n",
    "    }).to(device)\n",
    "\n",
    "    # Set up data loaders\n",
    "    data_module = DataModule(train_data, seq_length=10, batch_size=config[\"batch_size\"])\n",
    "    train_loader = data_module.train_loader\n",
    "    val_loader = data_module.eval_loader\n",
    "    test_loader = data_module.test_loader\n",
    "\n",
    "    # Set up loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Limit epochs for tuning\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Report metrics to Ray Tune\n",
    "        session.report({\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_loss\": val_loss / len(val_loader),\n",
    "            \"train_accuracy\": train_correct / train_total,\n",
    "            \"train_loss\": train_loss / len(train_loader),\n",
    "            \"epoch\": epoch\n",
    "        })\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebdbbfb01e2f4af6",
   "metadata": {},
   "source": [
    "# %%\n",
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"hidden_size\": tune.choice([32, 64, 128, 256]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-3),\n",
    "    \"batch_size\": tune.choice([16, 32, 64, 128])\n",
    "}\n",
    "\n",
    "# Configure the ASHA scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=10,  # Maximum number of epochs\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        partial(\n",
    "            train_lstm,\n",
    "            input_size=33,\n",
    "            num_classes=3,\n",
    "            train_data=training_data,\n",
    "            val_data=None,\n",
    "            test_data=None\n",
    "        ),\n",
    "        resources={\"cpu\": 2, \"gpu\": 0}  # Adjust based on your hardware\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        scheduler=scheduler,\n",
    "        num_samples=50,  # Number of hyperparameter combinations to try\n",
    "        trial_dirname_creator=lambda trial: f\"{trial.trainable_name}_{trial.trial_id[:4]}\"\n",
    "    ),\n",
    "    param_space=config\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "results = tuner.fit()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1dd602774859b1d0",
   "metadata": {},
   "source": [
    "# %%\n",
    "# Get the best hyperparameters\n",
    "best_result = results.get_best_result(\"val_accuracy\", \"max\")\n",
    "best_config = best_result.config\n",
    "print(\"Best config:\", best_config)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_lr = best_config[\"lr\"]\n",
    "best_hidden_size = best_config[\"hidden_size\"]\n",
    "best_num_layers = best_config[\"num_layers\"]\n",
    "best_dropout = best_config[\"dropout_rate\"]\n",
    "best_weight_decay = best_config[\"weight_decay\"]\n",
    "best_batch_size = best_config[\"batch_size\"]\n",
    "\n",
    "# Plot results\n",
    "df_results = results.get_dataframe()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot learning rate vs validation accuracy\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(df_results[\"config/lr\"], df_results[\"val_accuracy\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "# Plot hidden size vs validation accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(df_results[\"config/hidden_size\"], df_results[\"val_accuracy\"])\n",
    "plt.xlabel(\"Hidden Size\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "# Plot num_layers vs validation accuracy\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df_results[\"config/num_layers\"], df_results[\"val_accuracy\"])\n",
    "plt.xlabel(\"Number of Layers\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "# Plot dropout vs validation accuracy\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df_results[\"config/dropout_rate\"], df_results[\"val_accuracy\"])\n",
    "plt.xlabel(\"Dropout Rate\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "# Plot weight decay vs validation accuracy\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(df_results[\"config/weight_decay\"], df_results[\"val_accuracy\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Weight Decay\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "# Plot batch size vs validation accuracy\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df_results[\"config/batch_size\"], df_results[\"val_accuracy\"])\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd0e9e8dfd292942",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8b0067a4686aadf",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62964a9774211cfa",
   "metadata": {},
   "source": [
    "#ticker.get_balance_sheet(freq='quarterly')\n",
    "#ticker.get_calendar()\n",
    "#ticker.get_cash_flow(freq='quarterly')\n",
    "#earnings_data = ticker.get_earnings_dates()\n",
    "#income_statement = ticker.get_income_stmt(freq='yearly').T\n",
    "#ticker.get_institutional_holders()\n",
    "#ticker.get_recommendations()\n",
    "#ticker.get_sustainability()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "387ef9179077621f",
   "metadata": {},
   "source": [
    "# define a function to fetch the options data for a given ticker symbol\n",
    "#def fetch_options_data(ticker_symbol):\n",
    "    #ticker = yf.Ticker(ticker_symbol)\n",
    "#    options_dates = ticker.options\n",
    "#    options_data = ticker.option_chain(date='2025-03-21')\n",
    "#    return options_data.calls, options_data.puts\n",
    "##ionq_stock_data = ionq_stock_data.sort_values(by='Date', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
